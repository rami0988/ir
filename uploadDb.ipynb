{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8994b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\lenovo\\desktop\\ir\\ir_v5\\venv\\lib\\site-packages (4.13.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\lenovo\\desktop\\ir\\ir_v5\\venv\\lib\\site-packages (from pymongo) (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T09:38:01.797470Z",
     "start_time": "2025-06-22T09:30:57.792631Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB successfully\n",
      "Uploaded file tfidf_vectorizer.joblib to antique.representation using GridFS\n",
      "Uploaded file tfidf_matrix.joblib to antique.representation using GridFS\n",
      "Uploaded file embeddings_matrix.joblib to antique.representation using GridFS\n",
      "Uploaded file embeddings_vectorizer.joblib to antique.representation using GridFS\n",
      "Uploaded file embedding_index.faiss to antique.representation using GridFS\n",
      "Closed MongoDB connection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DocumentTooLarge\n",
    "from gridfs import GridFS\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# إعداد السجل (Logging)\n",
    "if not os.path.exists(\"logs\"):\n",
    "    os.makedirs(\"logs\")\n",
    "log_file = f\"logs/mongo_upload_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "def connect_to_mongo():\n",
    "    try:\n",
    "        connection_string = \"mongodb://localhost:27017/\"\n",
    "        client = MongoClient(connection_string)\n",
    "        client.admin.command('ping')\n",
    "        logging.info(\"Connected to MongoDB successfully\")\n",
    "        print(\"Connected to MongoDB successfully\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to MongoDB: {str(e)}\")\n",
    "        print(f\"Failed to connect to MongoDB: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            content = df.to_dict('records')  # تخزين كسجلات لقواعد البيانات\n",
    "        elif file_name.endswith(('.joblib', '.faiss')):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                content = file.read()  # بيانات ثنائية\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "        return {\"file_name\": file_name, \"content\": content}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def upload_file_to_mongo(file_path, db_name, collection_name, client):\n",
    "    try:\n",
    "        document = read_file_content(file_path)\n",
    "        if document is None:\n",
    "            return\n",
    "\n",
    "        db = client[db_name]\n",
    "\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # نحدد حجم الوثيقة النصية (لو > 16MB نستخدم GridFS)\n",
    "        content_size = len(document[\"content\"]) if isinstance(document[\"content\"], (bytes, str)) else 0\n",
    "        if file_name.endswith(('.joblib', '.faiss')) or content_size > 16 * 1024 * 1024:\n",
    "            # رفع باستخدام GridFS\n",
    "            fs = GridFS(db, collection=collection_name)\n",
    "            # تحقق هل الملف موجود مسبقاً بنفس الاسم وحذفه\n",
    "            existing = fs.find_one({\"filename\": file_name})\n",
    "            if existing:\n",
    "                fs.delete(existing._id)\n",
    "                logging.info(f\"Deleted existing GridFS file {file_name} before upload\")\n",
    "\n",
    "            with open(file_path, 'rb') as file:\n",
    "                fs.put(file, filename=file_name, metadata={\"dataset_name\": db_name, \"file_label\": collection_name})\n",
    "\n",
    "            logging.info(f\"Uploaded file {file_name} to {db_name}.{collection_name} using GridFS\")\n",
    "            print(f\"Uploaded file {file_name} to {db_name}.{collection_name} using GridFS\")\n",
    "        else:\n",
    "            # إدخال الوثيقة العادية بعد حذف الوثيقة بنفس الاسم إن وجدت\n",
    "            collection = db[collection_name]\n",
    "            existing = collection.find_one({\"file_name\": file_name})\n",
    "            if existing:\n",
    "                collection.delete_one({\"_id\": existing[\"_id\"]})\n",
    "                logging.info(f\"Deleted existing document {file_name} before upload\")\n",
    "\n",
    "            # نضيف معلومات إضافية من dataset_name و collection_name\n",
    "            doc_to_insert = {\n",
    "                \"file_name\": file_name,\n",
    "                \"content\": document[\"content\"],\n",
    "                \"dataset_name\": db_name,\n",
    "                \"file_label\": collection_name\n",
    "            }\n",
    "            collection.insert_one(doc_to_insert)\n",
    "\n",
    "            logging.info(f\"Uploaded file {file_name} to {db_name}.{collection_name}\")\n",
    "            print(f\"Uploaded file {file_name} to {db_name}.{collection_name}\")\n",
    "\n",
    "    except DocumentTooLarge as e:\n",
    "        logging.error(f\"Error uploading {file_path} to MongoDB: Document too large, consider using GridFS ({str(e)})\")\n",
    "        print(f\"Error uploading {file_path} to MongoDB: Document too large, consider using GridFS ({str(e)})\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading {file_path} to MongoDB: {str(e)}\")\n",
    "        print(f\"Error uploading {file_path} to MongoDB: {str(e)}\")\n",
    "\n",
    "def upload_files():\n",
    "    files_to_upload = [\n",
    "        (\"data/antique/tfidf_vectorizer.joblib\", \"antique\", \"representation\"),\n",
    "        (\"data/antique/tfidf_matrix.joblib\", \"antique\", \"representation\"),\n",
    "        (\"data/antique/embeddings_matrix.joblib\", \"antique\", \"representation\"),\n",
    "        (\"data/antique/embeddings_vectorizer.joblib\", \"antique\", \"representation\"),\n",
    "        (\"data/antique/embedding_index.faiss\", \"antique\", \"representation\"),\n",
    "    ]\n",
    "\n",
    "    client = connect_to_mongo()\n",
    "    if not client:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for file_path, db_name, collection_name in files_to_upload:\n",
    "            absolute_path = os.path.abspath(file_path)\n",
    "            if not os.path.isfile(absolute_path):\n",
    "                print(f\"Error: File '{absolute_path}' does not exist.\")\n",
    "                logging.error(f\"File not found: {absolute_path}\")\n",
    "                continue\n",
    "            upload_file_to_mongo(absolute_path, db_name, collection_name, client)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in upload process: {str(e)}\")\n",
    "        print(f\"Error in upload process: {str(e)}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "        logging.info(\"Closed MongoDB connection\")\n",
    "        print(\"Closed MongoDB connection\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d78a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB successfully\n",
      "Uploaded file tfidf_vectorizer.joblib to beir.representation using GridFS\n",
      "Uploaded file tfidf_matrix.joblib to beir.representation using GridFS\n",
      "Uploaded file embeddings_matrix.joblib to beir.representation using GridFS\n",
      "Uploaded file embeddings_vectorizer.joblib to beir.representation using GridFS\n",
      "Uploaded file embedding_index.faiss to beir.representation using GridFS\n",
      "Closed MongoDB connection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DocumentTooLarge\n",
    "from gridfs import GridFS\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# إعداد السجل (Logging)\n",
    "if not os.path.exists(\"logs\"):\n",
    "    os.makedirs(\"logs\")\n",
    "log_file = f\"logs/mongo_upload_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "def connect_to_mongo():\n",
    "    try:\n",
    "        connection_string = \"mongodb://localhost:27017/\"\n",
    "        client = MongoClient(connection_string)\n",
    "        client.admin.command('ping')\n",
    "        logging.info(\"Connected to MongoDB successfully\")\n",
    "        print(\"Connected to MongoDB successfully\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to MongoDB: {str(e)}\")\n",
    "        print(f\"Failed to connect to MongoDB: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            content = df.to_dict('records')  # تخزين كسجلات لقواعد البيانات\n",
    "        elif file_name.endswith(('.joblib', '.faiss')):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                content = file.read()  # بيانات ثنائية\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "        return {\"file_name\": file_name, \"content\": content}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def upload_file_to_mongo(file_path, db_name, collection_name, client):\n",
    "    try:\n",
    "        document = read_file_content(file_path)\n",
    "        if document is None:\n",
    "            return\n",
    "\n",
    "        db = client[db_name]\n",
    "\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        content_size = len(document[\"content\"]) if isinstance(document[\"content\"], (bytes, str)) else 0\n",
    "        if file_name.endswith(('.joblib', '.faiss')) or content_size > 16 * 1024 * 1024:\n",
    "            fs = GridFS(db, collection=collection_name)\n",
    "            existing = fs.find_one({\"filename\": file_name})\n",
    "            if existing:\n",
    "                fs.delete(existing._id)\n",
    "                logging.info(f\"Deleted existing GridFS file {file_name} before upload\")\n",
    "\n",
    "            with open(file_path, 'rb') as file:\n",
    "                fs.put(file, filename=file_name, metadata={\"dataset_name\": db_name, \"file_label\": collection_name})\n",
    "\n",
    "            logging.info(f\"Uploaded file {file_name} to {db_name}.{collection_name} using GridFS\")\n",
    "            print(f\"Uploaded file {file_name} to {db_name}.{collection_name} using GridFS\")\n",
    "        else:\n",
    "            collection = db[collection_name]\n",
    "            existing = collection.find_one({\"file_name\": file_name})\n",
    "            if existing:\n",
    "                collection.delete_one({\"_id\": existing[\"_id\"]})\n",
    "                logging.info(f\"Deleted existing document {file_name} before upload\")\n",
    "\n",
    "            doc_to_insert = {\n",
    "                \"file_name\": file_name,\n",
    "                \"content\": document[\"content\"],\n",
    "                \"dataset_name\": db_name,\n",
    "                \"file_label\": collection_name\n",
    "            }\n",
    "            collection.insert_one(doc_to_insert)\n",
    "\n",
    "            logging.info(f\"Uploaded file {file_name} to {db_name}.{collection_name}\")\n",
    "            print(f\"Uploaded file {file_name} to {db_name}.{collection_name}\")\n",
    "\n",
    "    except DocumentTooLarge as e:\n",
    "        logging.error(f\"Error uploading {file_path} to MongoDB: Document too large, consider using GridFS ({str(e)})\")\n",
    "        print(f\"Error uploading {file_path} to MongoDB: Document too large, consider using GridFS ({str(e)})\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading {file_path} to MongoDB: {str(e)}\")\n",
    "        print(f\"Error uploading {file_path} to MongoDB: {str(e)}\")\n",
    "\n",
    "def upload_files():\n",
    "    files_to_upload = [\n",
    "        (\"data/beir/tfidf_vectorizer.joblib\", \"beir\", \"representation\"),\n",
    "        (\"data/beir/tfidf_matrix.joblib\", \"beir\", \"representation\"),\n",
    "        (\"data/beir/embeddings_matrix.joblib\", \"beir\", \"representation\"),\n",
    "        (\"data/beir/embeddings_vectorizer.joblib\", \"beir\", \"representation\"),\n",
    "        (\"data/beir/embedding_index.faiss\", \"beir\", \"representation\"),\n",
    "    ]\n",
    "\n",
    "    client = connect_to_mongo()\n",
    "    if not client:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for file_path, db_name, collection_name in files_to_upload:\n",
    "            absolute_path = os.path.abspath(file_path)\n",
    "            if not os.path.isfile(absolute_path):\n",
    "                print(f\"Error: File '{absolute_path}' does not exist.\")\n",
    "                logging.error(f\"File not found: {absolute_path}\")\n",
    "                continue\n",
    "            upload_file_to_mongo(absolute_path, db_name, collection_name, client)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in upload process: {str(e)}\")\n",
    "        print(f\"Error in upload process: {str(e)}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "        logging.info(\"Closed MongoDB connection\")\n",
    "        print(\"Closed MongoDB connection\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ad26bd594880c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T09:03:09.515967Z",
     "start_time": "2025-06-22T09:03:07.356498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in file: ['tfidf_matrix', 'vectorizer', 'doc_ids']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "file_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\"\n",
    "data = joblib.load(file_path)\n",
    "print(\"Keys in file:\", list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df4aa9eae7f5e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T09:04:35.218014Z",
     "start_time": "2025-06-22T09:03:35.459563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in file: ['doc_ids', 'embeddings_matrix', 'model_name']\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Bert\\beir\\quora\\test\\doc\\bert_embedding.joblib\"\n",
    "data = joblib.load(file_path)\n",
    "print(\"Keys in file:\", list(data.keys()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
