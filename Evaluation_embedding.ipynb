{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8f8004",
   "metadata": {},
   "source": [
    "embedding_beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load BEIR dataset files...\n",
      "Loaded docs_beir.csv, number of docs: 522931\n",
      "Built doc_id_mapping_beir with 522931 entries\n",
      "Saved doc_id_mapping.joblib\n",
      "Loading embeddings matrix...\n",
      "Embeddings matrix shape: (522931, 768)\n",
      "Loading queries and qrels CSV files...\n",
      "Loaded 10000 queries and 15675 qrels\n",
      "Loading query embeddings matrix...\n",
      "Query embeddings shape: (10000, 768)\n",
      "Starting batch processing of queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 79/79 [3:15:00<00:00, 148.11s/batch, elapsed=11700.8s, remaining=0.0s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing completed.\n",
      "beir - MAP: 0.6910, Precision@10: 0.1082, Recall@10: 0.8065, MRR: 0.7044\n",
      "\n",
      "Sample results for BEIR (first 2 queries):\n",
      "Query ID: 18595, Top Docs: ['141147', '509049', '357115', '349321', '182669', '58815', '11252', '11253', '77135', '87814']\n",
      "Query ID: 18740, Top Docs: ['18739', '238735', '229953', '157043', '295694', '39478', '39477', '29316', '386708', '337175']\n",
      "Garbage collection done.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# تحميل بيانات NLTK مرة واحدة\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# زيادة حجم الـ batch\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "base_path_beir = r\"data\\beir\"\n",
    "\n",
    "embeddings_matrix_path_beir = os.path.join(base_path_beir, \"embeddings_matrix.joblib\")\n",
    "query_embeddings_path_beir = os.path.join(base_path_beir, \"query_embeddings_matrix_beir.joblib\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"queries_beir.csv\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "docs_beir_csv_path = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "\n",
    "print(\"Starting to load BEIR dataset files...\")\n",
    "\n",
    "docs_beir = pd.read_csv(docs_beir_csv_path)\n",
    "print(f\"Loaded docs_beir.csv, number of docs: {len(docs_beir)}\")\n",
    "\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "print(f\"Built doc_id_mapping_beir with {len(doc_id_mapping_beir)} entries\")\n",
    "\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "print(\"Saved doc_id_mapping.joblib\")\n",
    "\n",
    "print(\"Loading embeddings matrix...\")\n",
    "embeddings_matrix_beir = joblib.load(embeddings_matrix_path_beir)\n",
    "print(f\"Embeddings matrix shape: {embeddings_matrix_beir.shape}\")\n",
    "\n",
    "print(\"Loading queries and qrels CSV files...\")\n",
    "queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "print(f\"Loaded {len(queries_df_beir)} queries and {len(qrels_df_beir)} qrels\")\n",
    "\n",
    "print(\"Loading query embeddings matrix...\")\n",
    "query_embeddings_beir = joblib.load(query_embeddings_path_beir)\n",
    "print(f\"Query embeddings shape: {query_embeddings_beir.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(batch_idx, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping):\n",
    "    batch_results = {}\n",
    "    for j, (query_embedding, query_id) in enumerate(zip(batch_queries, batch_query_ids)):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]\n",
    "        doc_scores = list(enumerate(similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_10_docs\n",
    "        del similarities\n",
    "        gc.collect()\n",
    "    return batch_results\n",
    "\n",
    "def process_similarities_in_batches(query_embeddings, doc_embeddings, queries_df, batch_size, doc_id_mapping):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    total_batches = len(batch_indices)\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(\n",
    "                executor.submit(process_batch, i, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping)\n",
    "            )\n",
    "\n",
    "        completed = 0\n",
    "        with tqdm(total=total_batches, desc=\"Processing queries\", dynamic_ncols=True, unit=\"batch\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_results = future.result()\n",
    "                results.update(batch_results)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time_per_batch = elapsed / completed\n",
    "                remaining = avg_time_per_batch * (total_batches - completed)\n",
    "                pbar.set_postfix({\n",
    "                    \"elapsed\": f\"{elapsed:.1f}s\",\n",
    "                    \"remaining\": f\"{remaining:.1f}s\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "# دوال التقييم\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "qrels_beir = load_qrels(qrels_df_beir)\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting batch processing of queries...\")\n",
    "results_beir = process_similarities_in_batches(\n",
    "    query_embeddings_beir, embeddings_matrix_beir, queries_df_beir, BATCH_SIZE, doc_id_mapping_beir\n",
    ")\n",
    "print(\"Batch processing completed.\")\n",
    "\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n",
    "\n",
    "print(\"\\nSample results for BEIR (first 2 queries):\")\n",
    "for query_id in list(results_beir.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_beir[query_id]}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"Garbage collection done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1beb5",
   "metadata": {},
   "source": [
    "embedding_ANTIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 11/11 [02:43<00:00, 14.90s/batch, elapsed=164.1s, remaining=0.0s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4011, Precision@10: 0.1659, Recall@10: 0.0543, MRR: 0.4514\n",
      "\n",
      "Sample results for ANTIQUE (first 2 queries):\n",
      "Query ID: 312215, Top Docs: ['3079985_9', '1060042_14', '312215_4', '1060042_20', '107187_3', '3530023_3', '560479_3', '1080427_6', '3005801_14', '312215_10']\n",
      "Query ID: 3363149, Top Docs: ['3363149_5', '2652261_7', '1552113_4', '937255_12', '4103061_4', '1611814_2', '2959818_4', '2957934_6', '1865077_2', '699780_7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# تحميل بيانات NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# المسارات\n",
    "base_path_antique = r\"data\\antique\"\n",
    "\n",
    "\n",
    "embeddings_matrix_path_antique = os.path.join(base_path_antique, \"embeddings_matrix.joblib\")\n",
    "\n",
    "query_embeddings_path_antique = os.path.join(base_path_antique, \"query_embeddings_matrix_antique.joblib\")\n",
    "\n",
    "queries_path_antique = os.path.join(base_path_antique, \"queries_antique.csv\")\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "\n",
    "\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "\n",
    "\n",
    "# تحميل البيانات\n",
    "docs_antique = pd.read_csv(os.path.join(base_path_antique, \"docs_antique.csv\"))\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "\n",
    "\n",
    "embeddings_matrix_antique = joblib.load(embeddings_matrix_path_antique)\n",
    "\n",
    "\n",
    "# تحميل الكويري و qrels\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_embeddings_antique = joblib.load(query_embeddings_path_antique)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(batch_idx, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping):\n",
    "    batch_results = {}\n",
    "    for j, (query_embedding, query_id) in enumerate(zip(batch_queries, batch_query_ids)):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]\n",
    "        doc_scores = list(enumerate(similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_10_docs\n",
    "        del similarities\n",
    "        gc.collect()\n",
    "    return batch_results\n",
    "\n",
    "def process_similarities_in_batches(query_embeddings, doc_embeddings, queries_df, batch_size, doc_id_mapping):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    total_batches = len(batch_indices)\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(\n",
    "                executor.submit(process_batch, i, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping)\n",
    "            )\n",
    "\n",
    "        completed = 0\n",
    "        with tqdm(total=total_batches, desc=\"Processing queries\", dynamic_ncols=True, unit=\"batch\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_results = future.result()\n",
    "                results.update(batch_results)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time_per_batch = elapsed / completed\n",
    "                remaining = avg_time_per_batch * (total_batches - completed)\n",
    "                pbar.set_postfix({\n",
    "                    \"elapsed\": f\"{elapsed:.1f}s\",\n",
    "                    \"remaining\": f\"{remaining:.1f}s\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "results_antique = process_similarities_in_batches(\n",
    "    query_embeddings_antique, embeddings_matrix_antique, queries_df_antique, BATCH_SIZE, doc_id_mapping_antique\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "qrels_antique = load_qrels(qrels_df_antique)\n",
    "\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSample results for ANTIQUE (first 2 queries):\")\n",
    "for query_id in list(results_antique.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_antique[query_id]}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bd2f7",
   "metadata": {},
   "source": [
    "embedding  and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c57488",
   "metadata": {},
   "source": [
    "EMBEDDING_VECTOR_STORE_BIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc244e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "beir FAISS+Cosine: 100%|██████████| 625/625 [06:07<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEIR -> MAP: 0.6897, P@10: 0.1080, R@10: 0.8063, MRR: 0.7023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "import faiss\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# المسارات\n",
    "\n",
    "base_path_beir = r\"data/beir\"\n",
    "\n",
    "\n",
    "query_embeddings_path_beir = os.path.join(base_path_beir, \"query_embeddings_matrix_beir.joblib\")\n",
    "\n",
    "\n",
    "queries_path_beir = os.path.join(base_path_beir, \"queries_beir.csv\")\n",
    "\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "# تحميل الكويريز و qrels\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "\n",
    "# تحميل تمثيلات الاستعلامات\n",
    "\n",
    "query_embeddings_beir = joblib.load(query_embeddings_path_beir)\n",
    "\n",
    "# تحميل تمثيلات الوثائق كاملة\n",
    "\n",
    "doc_embeddings_beir = joblib.load(os.path.join(base_path_beir, \"embeddings_matrix.joblib\"))\n",
    "\n",
    "# تحميل الفهرس FAISS\n",
    "\n",
    "faiss_index_beir = faiss.read_index(os.path.join(base_path_beir, \"embedding_index.faiss\"))\n",
    "\n",
    "# تحميل doc_id mapping\n",
    "\n",
    "\n",
    "doc_mapping_beir = pd.read_csv(os.path.join(base_path_beir, \"docs_beir.csv\"))\n",
    "doc_id_mapping_beir = {i: str(row['doc_id']) for i, row in doc_mapping_beir.iterrows()}\n",
    "\n",
    "\n",
    "def process_batch_with_faiss(batch_idx, batch_queries, batch_query_ids, full_doc_embeddings, faiss_index, doc_id_mapping, top_k=20):\n",
    "    batch_results = {}\n",
    "    for query_embedding, query_id in zip(batch_queries, batch_query_ids):\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        distances, indices = faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        selected_doc_embeddings = full_doc_embeddings[indices[0]]\n",
    "        similarities = cosine_similarity(query_vector, selected_doc_embeddings)[0]\n",
    "        doc_scores = list(zip(indices[0], similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_docs\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def process_similarities_with_faiss(query_embeddings, queries_df, batch_size, full_doc_embeddings, faiss_index, doc_id_mapping, dataset_name):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(executor.submit(\n",
    "                process_batch_with_faiss, i, batch_queries, batch_query_ids,\n",
    "                full_doc_embeddings, faiss_index, doc_id_mapping\n",
    "            ))\n",
    "\n",
    "        with tqdm(total=len(batch_indices), desc=f\"{dataset_name} FAISS+Cosine\", dynamic_ncols=True) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_result = future.result()\n",
    "                results.update(batch_result)\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "# نتائج باستخدام FAISS + Cosine\n",
    "\n",
    "\n",
    "results_beir = process_similarities_with_faiss(query_embeddings_beir, queries_df_beir, BATCH_SIZE,\n",
    "                                               doc_embeddings_beir, faiss_index_beir, doc_id_mapping_beir,\n",
    "                                               dataset_name=\"beir\")\n",
    "\n",
    "# تقييم\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    return sum(1 for d in retrieved_docs[:k] if d in relevant_docs and relevant_docs[d] > 0) / k\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    rel = sum(1 for score in relevant_docs.values() if score > 0)\n",
    "    return sum(1 for d in retrieved_docs[:k] if d in relevant_docs and relevant_docs[d] > 0) / rel if rel else 0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, d in enumerate(retrieved_docs, 1):\n",
    "        if d in relevant_docs and relevant_docs[d] > 0:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    rel, acc = 0, 0\n",
    "    for i, d in enumerate(retrieved_docs, 1):\n",
    "        if d in relevant_docs and relevant_docs[d] > 0:\n",
    "            rel += 1\n",
    "            acc += rel / i\n",
    "    return acc / rel if rel else 0\n",
    "\n",
    "def evaluate_results(results, qrels_df, queries_df, name):\n",
    "    qrels = load_qrels(qrels_df)\n",
    "    maps, precs, recalls, mrrs = [], [], [], []\n",
    "    for qid in results:\n",
    "        rel = qrels.get(qid, {})\n",
    "        res = results[qid]\n",
    "        maps.append(calculate_map(res, rel))\n",
    "        precs.append(calculate_precision_at_k(res, rel))\n",
    "        recalls.append(calculate_recall_at_k(res, rel))\n",
    "        mrrs.append(calculate_rr(res, rel))\n",
    "    print(f\"{name} -> MAP: {np.mean(maps):.4f}, P@10: {np.mean(precs):.4f}, R@10: {np.mean(recalls):.4f}, MRR: {np.mean(mrrs):.4f}\")\n",
    "\n",
    "\n",
    "evaluate_results(results_beir, qrels_df_beir, queries_df_beir, \"BEIR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d2766",
   "metadata": {},
   "source": [
    "EMBEDDING_VECTOR_STORE_ANTIQUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "antique FAISS+Cosine: 100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTIQUE -> MAP: 0.4011, P@10: 0.1659, R@10: 0.0543, MRR: 0.4514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "import faiss\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# المسارات\n",
    "base_path_antique = r\"data/antique\"\n",
    "\n",
    "\n",
    "query_embeddings_path_antique = os.path.join(base_path_antique, \"query_embeddings_matrix_antique.joblib\")\n",
    "\n",
    "\n",
    "queries_path_antique = os.path.join(base_path_antique, \"queries_antique.csv\")\n",
    "\n",
    "\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "\n",
    "# تحميل الكويريز و qrels\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "\n",
    "\n",
    "# تحميل تمثيلات الاستعلامات\n",
    "query_embeddings_antique = joblib.load(query_embeddings_path_antique)\n",
    "\n",
    "\n",
    "# تحميل تمثيلات الوثائق كاملة\n",
    "doc_embeddings_antique = joblib.load(os.path.join(base_path_antique, \"embeddings_matrix.joblib\"))\n",
    "\n",
    "\n",
    "# تحميل الفهرس FAISS\n",
    "faiss_index_antique = faiss.read_index(os.path.join(base_path_antique, \"embedding_index.faiss\"))\n",
    "\n",
    "\n",
    "# تحميل doc_id mapping\n",
    "doc_mapping_antique = pd.read_csv(os.path.join(base_path_antique, \"docs_antique.csv\"))\n",
    "doc_id_mapping_antique = {i: str(row['doc_id']) for i, row in doc_mapping_antique.iterrows()}\n",
    "\n",
    "\n",
    "def process_batch_with_faiss(batch_idx, batch_queries, batch_query_ids, full_doc_embeddings, faiss_index, doc_id_mapping, top_k=20):\n",
    "    batch_results = {}\n",
    "    for query_embedding, query_id in zip(batch_queries, batch_query_ids):\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        distances, indices = faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        selected_doc_embeddings = full_doc_embeddings[indices[0]]\n",
    "        similarities = cosine_similarity(query_vector, selected_doc_embeddings)[0]\n",
    "        doc_scores = list(zip(indices[0], similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_docs\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def process_similarities_with_faiss(query_embeddings, queries_df, batch_size, full_doc_embeddings, faiss_index, doc_id_mapping, dataset_name):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(executor.submit(\n",
    "                process_batch_with_faiss, i, batch_queries, batch_query_ids,\n",
    "                full_doc_embeddings, faiss_index, doc_id_mapping\n",
    "            ))\n",
    "\n",
    "        with tqdm(total=len(batch_indices), desc=f\"{dataset_name} FAISS+Cosine\", dynamic_ncols=True) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_result = future.result()\n",
    "                results.update(batch_result)\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "results_antique = process_similarities_with_faiss(query_embeddings_antique, queries_df_antique, BATCH_SIZE,\n",
    "                                                  doc_embeddings_antique, faiss_index_antique, doc_id_mapping_antique,\n",
    "                                                  dataset_name=\"antique\")\n",
    "\n",
    "\n",
    "# تقييم\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    return sum(1 for d in retrieved_docs[:k] if d in relevant_docs and relevant_docs[d] > 0) / k\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    rel = sum(1 for score in relevant_docs.values() if score > 0)\n",
    "    return sum(1 for d in retrieved_docs[:k] if d in relevant_docs and relevant_docs[d] > 0) / rel if rel else 0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, d in enumerate(retrieved_docs, 1):\n",
    "        if d in relevant_docs and relevant_docs[d] > 0:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    rel, acc = 0, 0\n",
    "    for i, d in enumerate(retrieved_docs, 1):\n",
    "        if d in relevant_docs and relevant_docs[d] > 0:\n",
    "            rel += 1\n",
    "            acc += rel / i\n",
    "    return acc / rel if rel else 0\n",
    "\n",
    "def evaluate_results(results, qrels_df, queries_df, name):\n",
    "    qrels = load_qrels(qrels_df)\n",
    "    maps, precs, recalls, mrrs = [], [], [], []\n",
    "    for qid in results:\n",
    "        rel = qrels.get(qid, {})\n",
    "        res = results[qid]\n",
    "        maps.append(calculate_map(res, rel))\n",
    "        precs.append(calculate_precision_at_k(res, rel))\n",
    "        recalls.append(calculate_recall_at_k(res, rel))\n",
    "        mrrs.append(calculate_rr(res, rel))\n",
    "    print(f\"{name} -> MAP: {np.mean(maps):.4f}, P@10: {np.mean(precs):.4f}, R@10: {np.mean(recalls):.4f}, MRR: {np.mean(mrrs):.4f}\")\n",
    "\n",
    "evaluate_results(results_antique, qrels_df_antique, queries_df_antique, \"ANTIQUE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1abdce",
   "metadata": {},
   "source": [
    "## EMBEDDING & QUERY_ANTIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing queries: 100%|██████████\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4033, Precision@10: 0.1720, Recall@10: 0.0577, MRR: 0.4586\n",
      "\n",
      "Sample results for ANTIQUE (first 2 queries):\n",
      "Query ID: 312215, Top Docs: ['2198611_11', '1573436_0', '1060042_28', '1060042_20', '2981567_2', '2753255_2', '1658000_4', '1932479_9', '2131723_9', '312215_4']\n",
      "Query ID: 3363149, Top Docs: ['2652261_7', '3363149_4', '2652261_6', '3766120_6', '3363149_5', '1081922_8', '1042498_4', '3886596_6', '4103061_4', '3491622_11']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# تحميل بيانات NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# المسارات\n",
    "base_path_antique = r\"data\\antique\"\n",
    "embeddings_matrix_path_antique = os.path.join(base_path_antique, \"embeddings_matrix.joblib\")\n",
    "queries_path_antique = os.path.join(base_path_antique, \"enhanced_queries_antique.csv\")\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "vectorizer_path = os.path.join(base_path_antique, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "# تحميل البيانات\n",
    "docs_antique = pd.read_csv(os.path.join(base_path_antique, \"docs_antique.csv\"))\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "embeddings_matrix_antique = joblib.load(embeddings_matrix_path_antique)\n",
    "\n",
    "# تحميل الكويري و qrels\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "\n",
    "# تحميل الموديل لتحويل النصوص إلى تمثيلات\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "# استخراج النصوص المعالجة من حقل processed_query\n",
    "queries_texts = queries_df_antique['processed_query'].tolist()\n",
    "\n",
    "# تحويل النصوص إلى تمثيلات باستخدام الموديل\n",
    "query_embeddings_antique = vectorizer.encode(queries_texts, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def process_batch(batch_idx, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping):\n",
    "    batch_results = {}\n",
    "    for j, (query_embedding, query_id) in enumerate(zip(batch_queries, batch_query_ids)):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]\n",
    "        doc_scores = list(enumerate(similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_10_docs\n",
    "        del similarities\n",
    "        gc.collect()\n",
    "    return batch_results\n",
    "\n",
    "def process_similarities_in_batches(query_embeddings, doc_embeddings, queries_df, batch_size, doc_id_mapping):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    total_batches = len(batch_indices)\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(\n",
    "                executor.submit(process_batch, i, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping)\n",
    "            )\n",
    "\n",
    "        completed = 0\n",
    "        with tqdm(total=total_batches, desc=\"Processing queries\", bar_format=\"{l_bar}{bar}\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_results = future.result()\n",
    "                results.update(batch_results)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time_per_batch = elapsed / completed\n",
    "                remaining = avg_time_per_batch * (total_batches - completed)\n",
    "                pbar.set_postfix({\n",
    "                    \"elapsed\": f\"{elapsed:.1f}s\",\n",
    "                    \"remaining\": f\"{remaining:.1f}s\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "results_antique = process_similarities_in_batches(\n",
    "    query_embeddings_antique, embeddings_matrix_antique, queries_df_antique, BATCH_SIZE, doc_id_mapping_antique\n",
    ")\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "qrels_antique = load_qrels(qrels_df_antique)\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['processed_query'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "\n",
    "# عرض عينات من النتائج\n",
    "print(\"\\nSample results for ANTIQUE (first 2 queries):\")\n",
    "for query_id in list(results_antique.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_antique[query_id]}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01935d4",
   "metadata": {},
   "source": [
    "## EMBEDDING & QUERY_BEIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f31c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beir - MAP: 0.7768, Precision@10: 0.1932, Recall@10: 0.3810, MRR: 0.5067\n",
      "\n",
      "Sample results for BEIR (first 2 queries):\n",
      "Query ID: 46, Top Docs: ['271267', '63734', '188177', '19261', '19262', '53617', '136358', '31586', '41567', '63729']\n",
      "Query ID: 187, Top Docs: ['68732', '321224', '258305', '258306', '434871', '311017', '202157', '144864', '360881', '346186']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# تحميل بيانات NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# المسارات\n",
    "base_path_beir = r\"data\\beir\"\n",
    "embeddings_matrix_path_beir = os.path.join(base_path_beir, \"embeddings_matrix.joblib\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"enhanced_queries_beir.csv\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "vectorizer_path = os.path.join(base_path_beir, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "# تحميل البيانات\n",
    "docs_beir = pd.read_csv(os.path.join(base_path_beir, \"docs_beir.csv\"))\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "embeddings_matrix_beir = joblib.load(embeddings_matrix_path_beir)\n",
    "\n",
    "# تحميل الكويري و qrels\n",
    "queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "\n",
    "# تحميل الموديل لتحويل النصوص إلى تمثيلات\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "# استخراج النصوص المعالجة من حقل processed_query\n",
    "queries_texts = queries_df_beir['processed_query'].tolist()\n",
    "\n",
    "# تحويل النصوص إلى تمثيلات باستخدام الموديل\n",
    "query_embeddings_beir = vectorizer.encode(queries_texts, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def process_batch(batch_idx, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping):\n",
    "    batch_results = {}\n",
    "    for j, (query_embedding, query_id) in enumerate(zip(batch_queries, batch_query_ids)):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]\n",
    "        doc_scores = list(enumerate(similarities))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "        batch_results[query_id] = top_10_docs\n",
    "        del similarities\n",
    "        gc.collect()\n",
    "    return batch_results\n",
    "\n",
    "def process_similarities_in_batches(query_embeddings, doc_embeddings, queries_df, batch_size, doc_id_mapping):\n",
    "    results = {}\n",
    "    batch_indices = list(range(0, len(query_embeddings), batch_size))\n",
    "    total_batches = len(batch_indices)\n",
    "    futures = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_embeddings[i:i + batch_size]\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(\n",
    "                executor.submit(process_batch, i, batch_queries, batch_query_ids, doc_embeddings, doc_id_mapping)\n",
    "            )\n",
    "\n",
    "        completed = 0\n",
    "        with tqdm(total=total_batches, desc=\"Processing queries\", bar_format=\"{l_bar}{bar}\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                batch_results = future.result()\n",
    "                results.update(batch_results)\n",
    "                completed += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time_per_batch = elapsed / completed\n",
    "                remaining = avg_time_per_batch * (total_batches - completed)\n",
    "                pbar.set_postfix({\n",
    "                    \"elapsed\": f\"{elapsed:.1f}s\",\n",
    "                    \"remaining\": f\"{remaining:.1f}s\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    return results\n",
    "\n",
    "results_beir = process_similarities_in_batches(\n",
    "    query_embeddings_beir, embeddings_matrix_beir, queries_df_beir, BATCH_SIZE, doc_id_mapping_beir\n",
    ")\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "qrels_beir = load_qrels(qrels_df_beir)\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['processed_query'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n",
    "\n",
    "# عرض عينات من النتائج\n",
    "print(\"\\nSample results for BEIR (first 2 queries):\")\n",
    "for query_id in list(results_beir.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_beir[query_id]}\")\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
