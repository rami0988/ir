{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtVxOnL8ZVO-",
        "outputId": "5b9b1321-0b86-4ef9-cf40-1d680d4b2f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m806.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Uninstall conflicting packages and clear cache\n",
        "!pip uninstall -y torch torchvision torchaudio sentence-transformers -q\n",
        "!pip cache purge -q\n",
        "\n",
        "# Install compatible versions quietly\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 sentence-transformers==2.7.0 pandas numpy joblib nltk tqdm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U_xwqEpAZXrq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import joblib\n",
        "import ast\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "172ul38fZdjp",
        "outputId": "fac0eee3-906d-4cf2-954d-77a9ce8b3627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK data quietly\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Mount Google Drive quietly\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q4SsDTOBiHx_",
        "outputId": "bea1d81d-0de9-452f-acca-a69799a0ff3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder 1wYN3g7t8PSWm4LcZeq5W6J3zesI2WSZX antique\n",
            "Retrieving folder 12vUDb_w60AQTRSi-EklbQzasw_W0mzu- logs\n",
            "Retrieving folder 1Sw3oWsItL5S9sx4Fk7Z2kflYnHVv6p_7 tfidf\n",
            "Processing file 1K7q-a957YqNpiuzaikCkgL21hsnVZ9DN doc_id_mapping.joblib\n",
            "Processing file 1Txlz1nxyR8AwkDwMgBtBymMTvFgyWyOV docs_antique.csv\n",
            "Processing file 1KppNyTl-06ZVF9BYGrfHOfphH8dHd_kw embedding_index.faiss\n",
            "Processing file 1cdpLRWB_mP8AkdhnajUhA2_IlVVjwl2W embeddings_matrix.joblib\n",
            "Processing file 1WvjaJEGgAz4bB1bqkUMenQH-ISZZ_YTv embeddings_vectorizer.joblib\n",
            "Processing file 14I2bjpIVGqTUGo4H0-HdTiiRc6Zh0cu0 enhanced_queries_antique.json\n",
            "Processing file 1m168ne8p049qOPrurWIntNu4nyocID2I index.db\n",
            "Processing file 19wRo_0iHxPMtXtlJ2yHM8hhdd89uJiq4 qrels_antique.csv\n",
            "Processing file 1U_OPYmDq0w1GLCnOSxYuPnU7nlChVwH6 queries_antique.csv\n",
            "Processing file 1p57TfQy_aWkcSLSlL7FsKPTG8gKslxeT query_embeddings.joblib\n",
            "Processing file 1z_lNnXe6l_Jek2mHjVo3CTSpF26ZVDkU query_enhancement_log.txt\n",
            "Processing file 1_baTh3Yko_YUfNx4Q-w8KtmgElPZySnQ read.py\n",
            "Processing file 1JGbtKe87zCXT3kymly_URuSonG4P7QMX tfidf_matrix.joblib\n",
            "Processing file 1RmehJ34s6lZDJT2unQn0unnGLJsR_QHj tfidf_vectorizer.joblib\n",
            "Processing file 1NcdKblkL_oAkfqTjQCHcmmY7HQoue6zs top_10_embeddings_results_antique.json\n",
            "Retrieving folder 1spa22az_DEwyAzVMIqLpuFJc4spuGLCC beir\n",
            "Retrieving folder 11RVmsWjqdEyLnSSKP-YtZx28xpDkY6S- logs\n",
            "Retrieving folder 10Fo7nrz8eer8IXevmePhDLyO5NHX8DgK tdidf\n",
            "Processing file 1Xh3B2Hkpyjvk_s1cttPcHxAcGd2iyLF9 doc_id_mapping.joblib\n",
            "Processing file 1A9He8Q8FLIJlHG2ORHG_b_v_Mr9tCAqa docs_beir.csv\n",
            "Processing file 1INxpEXjBbsK1H3A1EEwLKjNV8h9xLmN7 embedding_index.faiss\n",
            "Processing file 1BhBq-cWhLpB2aD_53iI2_i9w5yPhx0yU embeddings_matrix.joblib\n",
            "Processing file 1aK10vNCsxMNzyzG8pV0nCZC8XCxXPKBE embeddings_vectorizer.joblib\n",
            "Processing file 1DXw8lTPZtgvd004QQ1_uoSj1nsOIxXbg index.db\n",
            "Processing file 1jJytQfMGJ03DEsHBJRZkbAXOinHFrFq9 qrels_beir.csv\n",
            "Processing file 1Pw1enxt_y34M9GlOK0IavJQO1BCgY_kl queries_beir.csv\n",
            "Processing file 16no_52oaijTaZ-vYXTFu-drXSE3elmW- query_embeddings.joblib\n",
            "Processing file 1IcxToEug6vZcfvJ78OXJuIOF9ZLXr6ZO query_enhancement_log.txt\n",
            "Processing file 1mev6-ubiKtv57J65mMQNk0F-9PcevEvC read.py\n",
            "Processing file 1-UwhnEFv5_V8OYCK_y5gyptMUhGIfQvx tfidf_matrix.joblib\n",
            "Processing file 1rhxL6LdKf285-hLejyBW39rbo352VDKW tfidf_vectorizer.joblib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ فشل تحميل المجلد: Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1K7q-a957YqNpiuzaikCkgL21hsnVZ9DN\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "⚠️ يرجى تحميل المجلد يدويًا إلى '/content/data' من الرابط:\n",
            "https://drive.google.com/drive/folders/1Y19Gai5PCohj8Y4ny-gGHoXGIm0d7WJm\n",
            "تأكد من أن إعدادات المشاركة مضبوطة على 'أي شخص لديه الرابط'.\n",
            "✅ GPU متوفر: Tesla T4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔄 معالجة الاستعلامات: 100%|██████████| 20/20 [1:34:46<00:00, 284.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ تم حفظ 10000 استعلام إلى: /content/enhanced_queries_beir.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_67710b50-d4e6-4f14-8b3b-79dd1d8cb205\", \"enhanced_queries_beir.csv\", 1690365)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 تم تنزيل الملف: /content/enhanced_queries_beir.csv\n"
          ]
        }
      ],
      "source": [
        "# ✅ تثبيت gdown لتحميل الملفات من Google Drive\n",
        "!pip install -U gdown\n",
        "\n",
        "# ✅ تحميل المجلد من Google Drive (الرابط المشترك)\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "folder_id = \"1Y19Gai5PCohj8Y4ny-gGHoXGIm0d7WJm\"\n",
        "drive_path = \"/content/data\"\n",
        "\n",
        "try:\n",
        "    gdown.download_folder(f\"https://drive.google.com/drive/folders/{folder_id}\", output=drive_path, quiet=False, use_cookies=False)\n",
        "    print(f\"✅ تم تحميل المجلد إلى: {drive_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ فشل تحميل المجلد: {str(e)}\")\n",
        "    print(\"⚠️ يرجى تحميل المجلد يدويًا إلى '/content/data' من الرابط:\")\n",
        "    print(f\"https://drive.google.com/drive/folders/{folder_id}\")\n",
        "    print(\"تأكد من أن إعدادات المشاركة مضبوطة على 'أي شخص لديه الرابط'.\")\n",
        "\n",
        "# ✅ التحقق من وجود المجلد\n",
        "if not os.path.exists(drive_path):\n",
        "    raise FileNotFoundError(f\"❌ المجلد {drive_path} غير موجود! يرجى تحميل المجلد يدويًا.\")\n",
        "\n",
        "# ✅ باقي المكتبات\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import joblib\n",
        "import ast\n",
        "import re\n",
        "import gc\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "# ✅ تحميل بيانات NLTK بصمت\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# ✅ تحقق من GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"✅ GPU متوفر: {gpu_name}\")\n",
        "    if \"T4\" not in gpu_name:\n",
        "        print(\"⚠️ هذا ليس T4 GPU. قد تختلف السرعة حسب نوع البطاقة.\")\n",
        "else:\n",
        "    raise RuntimeError(\"❌ لا يوجد GPU! فعّل GPU من Runtime > Change runtime type.\")\n",
        "\n",
        "# ✅ دوال الموديل والمعالجة\n",
        "def custom_tokenizer(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2 and t not in ['example', 'test']]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
        "\n",
        "def preprocess_text(text, cache=None):\n",
        "    if cache is not None and text in cache:\n",
        "        return cache[text]\n",
        "    tokens = custom_tokenizer(text)\n",
        "    if cache is not None:\n",
        "        cache[text] = tokens\n",
        "    return tokens\n",
        "\n",
        "def load_embeddings(dataset_name, drive_path):\n",
        "    path = f\"{drive_path}/{dataset_name}/embeddings_matrix.joblib\"\n",
        "    return joblib.load(path) if os.path.exists(path) else np.array([])\n",
        "\n",
        "def load_doc_data(dataset_name, drive_path):\n",
        "    path = f\"{drive_path}/{dataset_name}/index.db\"\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    try:\n",
        "        conn = sqlite3.connect(path)\n",
        "        cur = conn.cursor()\n",
        "        cur.execute('SELECT doc_id, processed_text FROM documents')\n",
        "        data = {row[0]: row[1] for row in cur.fetchall()}\n",
        "        conn.close()\n",
        "        return data\n",
        "    except Exception:\n",
        "        if 'conn' in locals():\n",
        "            conn.close()\n",
        "        return {}\n",
        "\n",
        "def enhance_queries(queries, dataset_name, model, drive_path, doc_data,\n",
        "                    enhance=True, batch_size=16, chunk_size=1000):\n",
        "    if not enhance:\n",
        "        return queries, [preprocess_text(q) for q in queries]\n",
        "\n",
        "    query_embeddings = load_embeddings(dataset_name, drive_path)\n",
        "    if len(query_embeddings) == 0:\n",
        "        return queries, [preprocess_text(q) for q in queries]\n",
        "\n",
        "    mapping_file = f\"{drive_path}/{dataset_name}/doc_id_mapping.joblib\"\n",
        "    if not os.path.exists(mapping_file):\n",
        "        return queries, [preprocess_text(q) for q in queries]\n",
        "    index_to_doc_id = joblib.load(mapping_file)\n",
        "\n",
        "    token_cache = {}\n",
        "    processed_queries = [preprocess_text(q, token_cache) for q in queries]\n",
        "    query_texts = [' '.join(pq) for pq in processed_queries]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_vectors = model.encode(query_texts, convert_to_numpy=True, batch_size=batch_size, device=device)\n",
        "\n",
        "    query_vectors = torch.tensor(query_vectors, dtype=torch.float32).to(device)\n",
        "    query_embeddings = torch.tensor(query_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(0, query_embeddings.shape[0], chunk_size):\n",
        "        chunk = query_embeddings[i:i+chunk_size].to(device)\n",
        "        sim = torch.nn.functional.cosine_similarity(query_vectors.unsqueeze(1), chunk.unsqueeze(0), dim=2)\n",
        "        similarities.append(sim)\n",
        "        del chunk, sim\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    similarities = torch.cat(similarities, dim=1)\n",
        "    top_indices = torch.topk(similarities, k=5, dim=1)[1].cpu().numpy()\n",
        "    similarities = similarities.cpu()\n",
        "\n",
        "    enhanced_queries, final_processed_queries = [], []\n",
        "    for i, (query, processed_query) in enumerate(zip(queries, processed_queries)):\n",
        "        top_idx = top_indices[i]\n",
        "        similar_words, added = [], 0\n",
        "\n",
        "        for idx in top_idx:\n",
        "            doc_id = index_to_doc_id.get(idx)\n",
        "            if not doc_id or doc_id not in doc_data:\n",
        "                continue\n",
        "            try:\n",
        "                score = similarities[i][idx].item()\n",
        "                if 0.4 < score < 0.999:\n",
        "                    words = ast.literal_eval(doc_data[doc_id])\n",
        "                    if words:\n",
        "                        similar_words.extend(words)\n",
        "                        added += 1\n",
        "                        if added >= 5:\n",
        "                            break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        seen = set(processed_query)\n",
        "        new_words = [w for w in similar_words if w not in seen]\n",
        "        new_words = new_words[:4]\n",
        "        final_query = query + ' '.join(new_words) if new_words else query\n",
        "        enhanced_queries.append(final_query)\n",
        "        final_processed_queries.append(processed_query)\n",
        "\n",
        "    return enhanced_queries, final_processed_queries\n",
        "\n",
        "def process_csv(input_csv, output_csv, dataset_name, model, drive_path, batch_size=500):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if 'text' not in df.columns:\n",
        "        raise ValueError(\"CSV must have a 'text' column\")\n",
        "\n",
        "    all_queries = df['text'].tolist()\n",
        "    total = len(all_queries)\n",
        "\n",
        "    doc_data = load_doc_data(dataset_name, drive_path)\n",
        "\n",
        "    all_enhanced, all_tokens = [], []\n",
        "\n",
        "    for i in tqdm(range(0, total, batch_size), desc=\"🔄 معالجة الاستعلامات\"):\n",
        "        batch_queries = all_queries[i:i+batch_size]\n",
        "        enhanced, tokens = enhance_queries(batch_queries, dataset_name, model, drive_path, doc_data)\n",
        "        all_enhanced.extend(enhanced)\n",
        "        all_tokens.extend([' '.join(t) for t in tokens])\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    df['enhanced_query'] = all_enhanced\n",
        "    df['processed_tokens'] = all_tokens\n",
        "    df.to_csv(output_csv, index=False)  # السطر المصحح\n",
        "    print(f\"\\n✅ تم حفظ {len(df)} استعلام إلى: {output_csv}\")\n",
        "\n",
        "# ✅ التنفيذ الرئيسي\n",
        "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
        "dataset_name = \"beir\"\n",
        "drive_path = \"/content/data\"\n",
        "input_csv = f\"{drive_path}/{dataset_name}/queries_beir.csv\"\n",
        "output_csv = \"/content/enhanced_queries_beir.csv\"\n",
        "\n",
        "process_csv(input_csv, output_csv, dataset_name, model, drive_path, batch_size=500)\n",
        "\n",
        "# ✅ تنزيل الناتج\n",
        "from google.colab import files\n",
        "if os.path.exists(output_csv):\n",
        "    files.download(output_csv)\n",
        "    print(f\"📥 تم تنزيل الملف: {output_csv}\")\n",
        "else:\n",
        "    print(f\"❌ الملف {output_csv} غير موجود!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
