{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69e5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoding queries for antique: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings saved: (176, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding queries for beir: 100%|██████████| 4/4 [00:22<00:00,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings saved: (10000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_model_from_disk(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"❌ Model not found at {model_path}\")\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "\n",
    "def load_queries_from_csv(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {file_path}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'text' not in df.columns:\n",
    "        raise ValueError(\"❌ CSV must contain a 'text' column\")\n",
    "    \n",
    "    return df['text'].tolist()\n",
    "\n",
    "\n",
    "def create_query_embeddings(query_csv_path, dataset_name, batch_size=3000):\n",
    "    try:\n",
    "        output_dir = f\"data/{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        embeddings_file = os.path.join(output_dir, \"query_embeddings_matrix.joblib\")\n",
    "        vectorizer_path = os.path.join(output_dir, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "        if os.path.exists(embeddings_file):\n",
    "            print(f\"✔️ Embeddings already exist for {dataset_name}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # تحميل الكويريز\n",
    "        queries = load_queries_from_csv(query_csv_path)\n",
    "        if not queries:\n",
    "            print(\"⚠️ No queries found.\")\n",
    "            return\n",
    "\n",
    "        # تحميل الموديل من الملف المحفوظ\n",
    "        model = load_model_from_disk(vectorizer_path)\n",
    "        embeddings = []\n",
    "\n",
    "        total_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "        with tqdm(total=total_batches, desc=f\"Encoding queries for {dataset_name}\") as pbar:\n",
    "            for i in range(0, len(queries), batch_size):\n",
    "                batch = queries[i:i + batch_size]\n",
    "                batch_embeddings = model.encode(batch, convert_to_numpy=True)\n",
    "                embeddings.append(batch_embeddings)\n",
    "                pbar.update(1)\n",
    "\n",
    "        embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "        # حفظ النتائج فقط\n",
    "        joblib.dump(embeddings, embeddings_file)\n",
    "\n",
    "        print(f\"✅ Embeddings saved: {embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "\n",
    "# التشغيل\n",
    "if __name__ == \"__main__\":\n",
    "    create_query_embeddings(r\"data\\antique\\queries_antique.csv\", \"antique\")\n",
    "    create_query_embeddings(r\"data\\beir\\queries_beir.csv\", \"beir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3944cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoding queries for antique:   1%|▏         | 2/135 [01:33<1:43:47, 46.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# التشغيل\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mcreate_query_embeddings_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mantique\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     create_query_embeddings_from_db(\u001b[33m\"\u001b[39m\u001b[33mbeir\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcreate_query_embeddings_from_db\u001b[39m\u001b[34m(dataset_name, batch_size)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(queries), batch_size):\n\u001b[32m     56\u001b[39m     batch = queries[i:i + batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     batch_embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     embeddings.append(batch_embeddings)\n\u001b[32m     59\u001b[39m     pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:720\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    718\u001b[39m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[32m    719\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m                 embeddings = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m         all_embeddings.extend(embeddings)\n\u001b[32m    724\u001b[39m all_embeddings = [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_model_from_disk(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"❌ Model not found at {model_path}\")\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "def load_documents_emb(dataset_name):\n",
    "    try:\n",
    "        db_path = f\"data/{dataset_name}/index.db\"\n",
    "        if not os.path.exists(db_path):\n",
    "            print(f\"❌ Database not found at {db_path}\")\n",
    "            return []\n",
    "\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT processed_text FROM documents\")\n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        documents = [row[0] for row in rows if row[0]]  # تأكد أنه غير فارغ\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading documents: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_query_embeddings_from_db(dataset_name, batch_size=3000):\n",
    "    try:\n",
    "        output_dir = f\"data/{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        embeddings_file = os.path.join(output_dir, \"query_embeddings_matrix.joblib\")\n",
    "        vectorizer_path = os.path.join(output_dir, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "        if os.path.exists(embeddings_file):\n",
    "            print(f\"✔️ Embeddings already exist for {dataset_name}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # تحميل النصوص المعالجة من قاعدة البيانات\n",
    "        queries = load_documents_emb(dataset_name)\n",
    "        if not queries:\n",
    "            print(f\"⚠️ No processed texts found in database for {dataset_name}.\")\n",
    "            return\n",
    "\n",
    "        # تحميل النموذج\n",
    "        model = load_model_from_disk(vectorizer_path)\n",
    "        embeddings = []\n",
    "\n",
    "        total_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "        with tqdm(total=total_batches, desc=f\"Encoding queries for {dataset_name}\") as pbar:\n",
    "            for i in range(0, len(queries), batch_size):\n",
    "                batch = queries[i:i + batch_size]\n",
    "                batch_embeddings = model.encode(batch, convert_to_numpy=True)\n",
    "                embeddings.append(batch_embeddings)\n",
    "                pbar.update(1)\n",
    "\n",
    "        embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "        # حفظ التضمينات\n",
    "        joblib.dump(embeddings, embeddings_file)\n",
    "        print(f\"✅ Embeddings saved: {embeddings.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# التشغيل\n",
    "if __name__ == \"__main__\":\n",
    "    create_query_embeddings_from_db(\"antique\")\n",
    "    create_query_embeddings_from_db(\"beir\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
