{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54841265",
   "metadata": {},
   "source": [
    "tfidf with inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing batches: 100%|██████████\n",
      "Processing batches: 100%|██████████\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4816, Precision@10: 0.2645, Recall@10: 0.0894, MRR: 0.5624\n",
      "beir - MAP: 0.5414, Precision@10: 0.0867, Recall@10: 0.6634, MRR: 0.5513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.5413612826436129),\n",
       " np.float64(0.08673),\n",
       " np.float64(0.6633663701409881),\n",
       " np.float64(0.5513044047619047))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "# تحميل بيانات NLTK المطلوبة\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# تعريف دالة التوكنايزر المخصصة\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# دالة محسنة للحصول على doc_ids من inverted index باستخدام استعلام واحد\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    if not processed_query:\n",
    "        return set()\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path, uri=True)\n",
    "        cursor = conn.cursor()\n",
    "        placeholders = ','.join('?' for _ in processed_query)\n",
    "        cursor.execute(f\"SELECT doc_id FROM inverted_index WHERE term IN ({placeholders})\", processed_query)\n",
    "        doc_ids = set(row[0] for row in cursor.fetchall())\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\") \n",
    "        doc_ids = set()\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# دالة لمعالجة دفعة واحدة من الاستعلامات\n",
    "def process_batch(batch_idx, batch_queries, batch_query_ids, batch_texts, doc_tfidf, doc_id_mapping, index_db_path):\n",
    "    batch_results = {}\n",
    "    try:\n",
    "        for j, (query_vector, query_id, query_text) in enumerate(zip(batch_queries, batch_query_ids, batch_texts)):\n",
    "            try:\n",
    "                processed_terms = custom_tokenizer(query_text)\n",
    "                doc_ids = get_doc_ids_from_index(processed_terms, index_db_path)\n",
    "\n",
    "                if not doc_ids:\n",
    "                    batch_results[query_id] = []\n",
    "                    continue\n",
    "\n",
    "                filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "                if not filtered_indices:\n",
    "                    batch_results[query_id] = []\n",
    "                    continue\n",
    "\n",
    "                filtered_doc_matrix = doc_tfidf[filtered_indices]\n",
    "                query_vector = query_vector.reshape(1, -1)\n",
    "                similarities = cosine_similarity(query_vector, filtered_doc_matrix)\n",
    "                doc_scores = list(zip(filtered_indices, similarities[0]))\n",
    "                doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "                batch_results[query_id] = top_10_docs\n",
    "\n",
    "                del similarities, filtered_doc_matrix\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query {query_id}: {e}\")  # استبدال logger.error\n",
    "                batch_results[query_id] = []\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {batch_idx}: {e}\")  # استبدال logger.error\n",
    "        return {}\n",
    "    return batch_results\n",
    "\n",
    "# دالة محسنة لمعالجة التشابه باستخدام المعالجة الموازية\n",
    "def process_tfidf_similarities_in_batches(query_tfidf, doc_tfidf, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    batch_indices = list(range(0, query_tfidf.shape[0], batch_size))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in batch_indices:\n",
    "            batch_queries = query_tfidf[i:i + batch_size]\n",
    "            batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "            batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "            futures.append(\n",
    "                executor.submit(process_batch, i, batch_queries, batch_query_ids, batch_texts, doc_tfidf, doc_id_mapping, index_db_path)\n",
    "            )\n",
    "        \n",
    "        for future in tqdm(futures, desc=\"Processing batches\", bar_format=\"{l_bar}{bar}\"):\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                results.update(batch_results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in future: {e}\")  # استبدال logger.error\n",
    "    \n",
    "    return results\n",
    "\n",
    "# دوال لحساب المقاييس\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []  # قائمة لتخزين قيم RR\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))  # إضافة RR\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0  # حساب MRR\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, \"f\"Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "    \n",
    "    \n",
    "    return map_score, precision_score, recall_score, mrr_score\n",
    "# مسارات الملفات وقواعد البيانات\n",
    "base_path_antique = r\"data\\antique\"\n",
    "base_path_beir = r\"data\\beir\"\n",
    "\n",
    "tfidf_matrix_path_antique = os.path.join(base_path_antique, \"tfidf_matrix.joblib\")\n",
    "queries_path_antique = os.path.join(base_path_antique, \"queries_antique.csv\")\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "tfidf_matrix_path_beir = os.path.join(base_path_beir, \"tfidf_matrix.joblib\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"queries_beir.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "# تحميل المستندات لإنشاء mapping بين index و doc_id\n",
    "try:\n",
    "    docs_antique = pd.read_csv(docs_path_antique)\n",
    "    docs_beir = pd.read_csv(docs_path_beir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "\n",
    "# تحميل مصفوفات TF-IDF\n",
    "try:\n",
    "    tfidf_matrix_antique = joblib.load(tfidf_matrix_path_antique)\n",
    "    tfidf_matrix_beir = joblib.load(tfidf_matrix_path_beir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading TF-IDF matrices: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "# تحميل ملفات الاستعلامات ومرجع التقييم\n",
    "try:\n",
    "    queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "    queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "    qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "    qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "# إزالة الاستعلامات الفارغة أو التي لا تحتوي نص\n",
    "queries_df_antique = queries_df_antique[queries_df_antique['text'].notna() & queries_df_antique['text'].str.strip() != '']\n",
    "queries_df_beir = queries_df_beir[queries_df_beir['text'].notna() & queries_df_beir['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "# تحميل محولات TF-IDF\n",
    "try:\n",
    "    tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "    tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading TF-IDF vectorizers: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "# تحويل نصوص الاستعلام إلى تمثيلات TF-IDF\n",
    "query_texts_antique = queries_df_antique['text'].tolist()\n",
    "query_tfidf_antique = tfidf_vectorizer_antique.transform(query_texts_antique)\n",
    "query_texts_beir = queries_df_beir['text'].tolist()\n",
    "query_tfidf_beir = tfidf_vectorizer_beir.transform(query_texts_beir)\n",
    "\n",
    "# حجم دفعة المعالجة\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# معالجة بيانات ANTIQUE\n",
    "try:\n",
    "    results_antique = process_tfidf_similarities_in_batches(\n",
    "        query_tfidf_antique, tfidf_matrix_antique, queries_df_antique,\n",
    "        BATCH_SIZE, doc_id_mapping_antique, base_path_antique\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error processing ANTIQUE dataset: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "# معالجة بيانات BEIR\n",
    "try:\n",
    "    results_beir = process_tfidf_similarities_in_batches(\n",
    "        query_tfidf_beir, tfidf_matrix_beir, queries_df_beir,\n",
    "        BATCH_SIZE, doc_id_mapping_beir, base_path_beir\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error processing BEIR dataset: {e}\")  # استبدال logger.error\n",
    "    raise\n",
    "\n",
    "# تحميل qrels\n",
    "qrels_antique = load_qrels(qrels_df_antique)\n",
    "qrels_beir = load_qrels(qrels_df_beir)\n",
    "\n",
    "# تقييم النتائج\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9ae48",
   "metadata": {},
   "source": [
    "## TFIDF QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing antique queries: 100%|██████████ | 00:00\n",
      "Processing beir queries: 100%|██████████ | 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.5260, Precision@10: 0.3119, Recall@10: 0.1045, MRR: 0.5823\n",
      "beir - MAP: 0.5496, Precision@10: 0.1328, Recall@10: 0.7462, MRR: 0.5712\n",
      "\n",
      "Sample results for ANTIQUE (first 2 queries):\n",
      "Query ID: 3990512, Top Docs: ['4087614_7', '4442135_2', '2960956_7', '1253368_5', '245498_1', '245498_7', '4099979_6', '4366141_8', '4009307_3', '506361_0']\n",
      "Query ID: 714612, Top Docs: ['714612_0', '1136432_0', '3069999_1', '1178326_1', '817286_2', '1802263_13', '3561080_2', '1369513_10', '714612_7', '714612_1']\n",
      "\n",
      "Sample results for BEIR (first 2 queries):\n",
      "Query ID: 46, Top Docs: ['167865', '188177', '174045', '147009', '41567', '19261', '69441', '47309', '5632', '456168']\n",
      "Query ID: 187, Top Docs: ['504714', '68733', '258306', '188', '171059', '68732', '144864', '360881', '58103', '225197']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# تحميل بيانات NLTK المطلوبة\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenizer مخصص\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2 and token not in ['example', 'test']]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def process_tfidf_similarities_in_batches(queries_df, tfidf_vectorizer, doc_tfidf, batch_size, doc_id_mapping, dataset_base_path, dataset_name, enhance=False):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    total_queries = len(queries_df)\n",
    "\n",
    "    for i in tqdm(range(0, total_queries, batch_size), \n",
    "              desc=f\"Processing {dataset_name} queries\", \n",
    "              total=(total_queries + batch_size - 1) // batch_size, \n",
    "              bar_format=\"{l_bar}{bar} | {remaining}\"):\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_text, query_id in zip(batch_texts, batch_query_ids):\n",
    "            query_to_process = query_text\n",
    "            processed_terms = custom_tokenizer(query_text)\n",
    "\n",
    "            query_vector = tfidf_vectorizer.transform([query_text])\n",
    "            doc_ids = get_doc_ids_from_index(processed_terms, index_db_path)\n",
    "\n",
    "            if not doc_ids:\n",
    "                results[query_id] = []\n",
    "                continue\n",
    "\n",
    "            filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "            if not filtered_indices:\n",
    "                results[query_id] = []\n",
    "                continue\n",
    "\n",
    "            filtered_doc_matrix = doc_tfidf[filtered_indices]\n",
    "            similarities = cosine_similarity(query_vector, filtered_doc_matrix)\n",
    "            doc_scores = list(zip(filtered_indices, similarities[0]))\n",
    "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "            results[query_id] = top_10_docs\n",
    "\n",
    "            del similarities, filtered_doc_matrix\n",
    "            gc.collect()\n",
    "\n",
    "    return results, {}\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores, precision_scores, recall_scores, rr_scores, low_map_queries = [], [], [], [], []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {np.mean(map_scores):.4f}, Precision@10: {np.mean(precision_scores):.4f}, Recall@10: {np.mean(recall_scores):.4f}, MRR: {np.mean(rr_scores):.4f}\")\n",
    "\n",
    "# تحميل المسارات\n",
    "base_path_antique = \"data/antique\"\n",
    "base_path_beir = \"data/beir\"\n",
    "\n",
    "queries_path_antique = os.path.join(base_path_antique, \"enhanced_queries_antique.csv\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"enhanced_queries_beir.csv\")\n",
    "\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "tfidf_matrix_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_matrix.joblib\"))\n",
    "tfidf_matrix_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_matrix.joblib\"))\n",
    "\n",
    "tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "queries_df_antique = pd.read_csv(queries_path_antique).dropna(subset=['text'])\n",
    "queries_df_beir = pd.read_csv(queries_path_beir).dropna(subset=['text']).head(1000)\n",
    "\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "\n",
    "qrels_antique = load_qrels(qrels_df_antique)\n",
    "qrels_beir = load_qrels(qrels_df_beir)\n",
    "\n",
    "docs_antique = pd.read_csv(docs_path_antique)\n",
    "docs_beir = pd.read_csv(docs_path_beir)\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "results_antique, _ = process_tfidf_similarities_in_batches(\n",
    "    queries_df_antique, tfidf_vectorizer_antique, tfidf_matrix_antique,\n",
    "    BATCH_SIZE, doc_id_mapping_antique, base_path_antique, \"antique\", enhance=False\n",
    ")\n",
    "\n",
    "results_beir, _ = process_tfidf_similarities_in_batches(\n",
    "    queries_df_beir, tfidf_vectorizer_beir, tfidf_matrix_beir,\n",
    "    BATCH_SIZE, doc_id_mapping_beir, base_path_beir, \"beir\", enhance=False\n",
    ")\n",
    "\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, \"antique\")\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, \"beir\")\n",
    "\n",
    "print(\"\\nSample results for ANTIQUE (first 2 queries):\")\n",
    "for query_id in list(results_antique.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_antique[query_id]}\")\n",
    "\n",
    "print(\"\\nSample results for BEIR (first 2 queries):\")\n",
    "for query_id in list(results_beir.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_beir[query_id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34ca73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Processing antique queries: 100%|██████████ | 00:00\n",
      "Processing beir queries: 100%|██████████ | 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.5260, Precision@10: 0.3119, Recall@10: 0.1045, MRR: 0.5823\n",
      "beir - MAP: 0.4949, Precision@10: 0.0946, Recall@10: 0.7102, MRR: 0.5049\n",
      "\n",
      "Sample results for ANTIQUE (first 2 queries):\n",
      "Query ID: 3990512, Top Docs: ['4087614_7', '4442135_2', '2960956_7', '1253368_5', '245498_1', '245498_7', '4099979_6', '4366141_8', '4009307_3', '506361_0']\n",
      "Query ID: 714612, Top Docs: ['714612_0', '1136432_0', '3069999_1', '1178326_1', '817286_2', '1802263_13', '3561080_2', '1369513_10', '714612_7', '714612_1']\n",
      "\n",
      "Sample results for BEIR (first 2 queries):\n",
      "Query ID: 46, Top Docs: ['300076', '19261', '53617', '75216', '104769', '48650', '301936', '353907', '93153', '296385']\n",
      "Query ID: 187, Top Docs: ['68732', '144864', '360881', '4789', '202157', '68733', '4788', '346186', '202765', '288149']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# تحميل بيانات NLTK المطلوبة\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenizer مخصص\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2 and token not in ['example', 'test']]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def process_tfidf_similarities_in_batches(queries_df, tfidf_vectorizer, doc_tfidf, batch_size, doc_id_mapping, dataset_base_path, dataset_name, enhance=False):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    total_queries = len(queries_df)\n",
    "\n",
    "    for i in tqdm(range(0, total_queries, batch_size), \n",
    "              desc=f\"Processing {dataset_name} queries\", \n",
    "              total=(total_queries + batch_size - 1) // batch_size, \n",
    "              bar_format=\"{l_bar}{bar} | {remaining}\"):\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_text, query_id in zip(batch_texts, batch_query_ids):\n",
    "            query_to_process = query_text\n",
    "            processed_terms = custom_tokenizer(query_text)\n",
    "\n",
    "            query_vector = tfidf_vectorizer.transform([query_text])\n",
    "            doc_ids = get_doc_ids_from_index(processed_terms, index_db_path)\n",
    "\n",
    "            if not doc_ids:\n",
    "                results[query_id] = []\n",
    "                continue\n",
    "\n",
    "            filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "            if not filtered_indices:\n",
    "                results[query_id] = []\n",
    "                continue\n",
    "\n",
    "            filtered_doc_matrix = doc_tfidf[filtered_indices]\n",
    "            similarities = cosine_similarity(query_vector, filtered_doc_matrix)\n",
    "            doc_scores = list(zip(filtered_indices, similarities[0]))\n",
    "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_10_docs = [doc_id_mapping[idx] for idx, _ in doc_scores[:10]]\n",
    "            results[query_id] = top_10_docs\n",
    "\n",
    "            del similarities, filtered_doc_matrix\n",
    "            gc.collect()\n",
    "\n",
    "    return results, {}\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores, precision_scores, recall_scores, rr_scores, low_map_queries = [], [], [], [], []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {np.mean(map_scores):.4f}, Precision@10: {np.mean(precision_scores):.4f}, Recall@10: {np.mean(recall_scores):.4f}, MRR: {np.mean(rr_scores):.4f}\")\n",
    "\n",
    "# تحميل المسارات\n",
    "base_path_antique = \"data/antique\"\n",
    "base_path_beir = \"data/beir\"\n",
    "\n",
    "queries_path_antique = os.path.join(base_path_antique, \"enhanced_queries_antique.csv\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"enhanced_queries_beir.csv\")\n",
    "\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "tfidf_matrix_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_matrix.joblib\"))\n",
    "tfidf_matrix_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_matrix.joblib\"))\n",
    "\n",
    "tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "queries_df_antique = pd.read_csv(queries_path_antique).dropna(subset=['text'])\n",
    "queries_df_beir = pd.read_csv(queries_path_beir).dropna(subset=['text'])\n",
    "\n",
    "qrels_df_antique = pd.read_csv(qrels_path_antique)\n",
    "qrels_df_beir = pd.read_csv(qrels_path_beir)\n",
    "\n",
    "qrels_antique = load_qrels(qrels_df_antique)\n",
    "qrels_beir = load_qrels(qrels_df_beir)\n",
    "\n",
    "docs_antique = pd.read_csv(docs_path_antique)\n",
    "docs_beir = pd.read_csv(docs_path_beir)\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "results_antique, _ = process_tfidf_similarities_in_batches(\n",
    "    queries_df_antique, tfidf_vectorizer_antique, tfidf_matrix_antique,\n",
    "    BATCH_SIZE, doc_id_mapping_antique, base_path_antique, \"antique\", enhance=False\n",
    ")\n",
    "\n",
    "results_beir, _ = process_tfidf_similarities_in_batches(\n",
    "    queries_df_beir, tfidf_vectorizer_beir, tfidf_matrix_beir,\n",
    "    BATCH_SIZE, doc_id_mapping_beir, base_path_beir, \"beir\", enhance=False\n",
    ")\n",
    "\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, \"antique\")\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, \"beir\")\n",
    "\n",
    "print(\"\\nSample results for ANTIQUE (first 2 queries):\")\n",
    "for query_id in list(results_antique.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_antique[query_id]}\")\n",
    "\n",
    "print(\"\\nSample results for BEIR (first 2 queries):\")\n",
    "for query_id in list(results_beir.keys())[:2]:\n",
    "    print(f\"Query ID: {query_id}, Top Docs: {results_beir[query_id]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
