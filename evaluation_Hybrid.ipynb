{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b703b0",
   "metadata": {},
   "source": [
    "HYBRID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97483d58",
   "metadata": {},
   "source": [
    "HYBRID_ANTIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef775354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "ğŸ”„ Processing queries (antique): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [06:35<00:00, 35.99s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4446, Precision@10: 0.2085, Recall@10: 0.0721, MRR: 0.4910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ BATCH_SIZE\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± Ø§Ù„Ù…Ø®ØµØµØ©\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ doc_ids Ù…Ù† inverted index\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "base_path_antique = r\"data\\antique\"\n",
    "\n",
    "\n",
    "tfidf_matrix_path_antique = os.path.join(base_path_antique, \"tfidf_matrix.joblib\")\n",
    "queries_path_antique = os.path.join(base_path_antique, \"queries_antique.csv\")\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_antique = os.path.join(base_path_antique, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_antique = os.path.join(base_path_antique, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ doc_id_mapping\n",
    "docs_antique = pd.read_csv(docs_path_antique)\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF Ùˆ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "tfidf_matrix_antique = joblib.load(tfidf_matrix_path_antique)\n",
    "\n",
    "embeddings_matrix_antique = joblib.load(embeddings_matrix_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "\n",
    "\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©\n",
    "queries_df_antique = queries_df_antique[queries_df_antique['text'].notna() & queries_df_antique['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„Ø§Øª TF-IDF\n",
    "tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ embeddings_vectorizer Ø§Ù„Ù…Ø®Ø²Ù†\n",
    "model_antique = joblib.load(embeddings_vectorizer_path_antique)\n",
    "\n",
    "\n",
    "\n",
    "query_texts_antique = queries_df_antique['text'].tolist()\n",
    "\n",
    "query_tfidf_antique = tfidf_vectorizer_antique.transform(query_texts_antique)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_embeddings_antique = joblib.load(os.path.join(base_path_antique, \"query_embeddings_matrix_antique.joblib\"))\n",
    "\n",
    "\n",
    "\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        \n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF ÙˆØ§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "\n",
    "        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… TF-IDF\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        filtered_embeddings = embeddings_matrix[filtered_indices]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, filtered_embeddings)\n",
    "        doc_indices_embedding = np.argsort(similarities_embedding[0])[::-1][:10]\n",
    "\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "        for idx in doc_indices_embedding:\n",
    "            original_idx = filtered_indices[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][idx] if idx < len(similarities_tfidf[0]) else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        \n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ranking error: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    \n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            if result['status'] == 'success':\n",
    "                results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']]\n",
    "            else:\n",
    "                results[query_id] = []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù‡Ø¬ÙŠÙ† ANTIQUE\n",
    "results_antique = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_antique, query_embeddings_antique, tfidf_matrix_antique, embeddings_matrix_antique,\n",
    "    queries_df_antique, BATCH_SIZE, doc_id_mapping_antique, base_path_antique\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels\n",
    "qrels_antique = load_qrels(pd.read_csv(qrels_path_antique))\n",
    "\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "del query_tfidf_antique,query_embeddings_antique, \n",
    "del tfidf_matrix_antique, embeddings_matrix_antique, \n",
    "del model_antique, \n",
    "gc.collect()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76feb2b9",
   "metadata": {},
   "source": [
    "HYBRID_BEIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb3f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11002]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11002]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ğŸ”„ Processing queries (beir): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [6:41:51<00:00, 38.58s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beir - MAP: 0.6929, Precision@10: 0.1088, Recall@10: 0.8113, MRR: 0.7059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ BATCH_SIZE\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± Ø§Ù„Ù…Ø®ØµØµØ©\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ doc_ids Ù…Ù† inverted index\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "\n",
    "base_path_beir = r\"data\\beir\"\n",
    "\n",
    "\n",
    "\n",
    "tfidf_matrix_path_beir = os.path.join(base_path_beir, \"tfidf_matrix.joblib\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"queries_beir.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_beir = os.path.join(base_path_beir, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_beir = os.path.join(base_path_beir, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ doc_id_mapping\n",
    "\n",
    "docs_beir = pd.read_csv(docs_path_beir)\n",
    "\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF Ùˆ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "\n",
    "tfidf_matrix_beir = joblib.load(tfidf_matrix_path_beir)\n",
    "\n",
    "embeddings_matrix_beir = joblib.load(embeddings_matrix_path_beir)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "\n",
    "queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©\n",
    "\n",
    "queries_df_beir = queries_df_beir[queries_df_beir['text'].notna() & queries_df_beir['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„Ø§Øª TF-IDF\n",
    "\n",
    "tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ embeddings_vectorizer Ø§Ù„Ù…Ø®Ø²Ù†\n",
    "\n",
    "model_beir = joblib.load(embeddings_vectorizer_path_beir)\n",
    "\n",
    "\n",
    "\n",
    "query_texts_beir = queries_df_beir['text'].tolist()\n",
    "\n",
    "query_tfidf_beir = tfidf_vectorizer_beir.transform(query_texts_beir)\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def encode_in_batches(texts, vectorizer, batch_size=BATCH_SIZE):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding queries\", bar_format=\"{l_bar}{bar}\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = vectorizer.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        gc.collect()\n",
    "    return np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "\n",
    "query_embeddings_beir = joblib.load(os.path.join(base_path_beir, \"query_embeddings_matrix_beir.joblib\"))\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù‡Ø¬ÙŠÙ†Ø© Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        \n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF ÙˆØ§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "\n",
    "        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… TF-IDF\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        filtered_embeddings = embeddings_matrix[filtered_indices]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, filtered_embeddings)\n",
    "        doc_indices_embedding = np.argsort(similarities_embedding[0])[::-1][:10]\n",
    "\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "        for idx in doc_indices_embedding:\n",
    "            original_idx = filtered_indices[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][idx] if idx < len(similarities_tfidf[0]) else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        \n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ranking error: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    \n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            if result['status'] == 'success':\n",
    "                results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']]\n",
    "            else:\n",
    "                results[query_id] = []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬     \n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "\n",
    "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù‡Ø¬ÙŠÙ† BEIR\n",
    "results_beir = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_beir, query_embeddings_beir, tfidf_matrix_beir, embeddings_matrix_beir,\n",
    "    queries_df_beir, BATCH_SIZE, doc_id_mapping_beir, base_path_beir\n",
    ")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels\n",
    "\n",
    "qrels_beir = load_qrels(pd.read_csv(qrels_path_beir))\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "del  query_tfidf_beir, query_embeddings_beir\n",
    "del  tfidf_matrix_beir , embeddings_matrix_beir\n",
    "del  model_beir\n",
    "gc.collect()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7e2c8",
   "metadata": {},
   "source": [
    "hybrid and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43166011",
   "metadata": {},
   "source": [
    "hybrid and vector store ANTIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\lenovo\\Desktop\\IR\\ir_v5\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ğŸ”„ Processing queries (antique): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [07:57<00:00, 43.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4267, Precision@10: 0.1585, Recall@10: 0.0516, MRR: 0.4628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ BATCH_SIZE\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± Ø§Ù„Ù…Ø®ØµØµØ©\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ doc_ids Ù…Ù† inverted index\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "base_path_antique = r\"data\\antique\"\n",
    "\n",
    "\n",
    "tfidf_matrix_path_antique = os.path.join(base_path_antique, \"tfidf_matrix.joblib\")\n",
    "queries_path_antique = os.path.join(base_path_antique, \"queries_antique.csv\")\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_antique = os.path.join(base_path_antique, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_antique = os.path.join(base_path_antique, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ doc_id_mapping\n",
    "docs_antique = pd.read_csv(docs_path_antique)\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF Ùˆ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "tfidf_matrix_antique = joblib.load(tfidf_matrix_path_antique)\n",
    "\n",
    "embeddings_matrix_antique = joblib.load(embeddings_matrix_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "queries_df_antique = pd.read_csv(queries_path_antique)\n",
    "\n",
    "\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©\n",
    "queries_df_antique = queries_df_antique[queries_df_antique['text'].notna() & queries_df_antique['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„Ø§Øª TF-IDF\n",
    "tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ embeddings_vectorizer Ø§Ù„Ù…Ø®Ø²Ù†\n",
    "model_antique = joblib.load(embeddings_vectorizer_path_antique)\n",
    "\n",
    "\n",
    "\n",
    "query_texts_antique = queries_df_antique['text'].tolist()\n",
    "\n",
    "query_tfidf_antique = tfidf_vectorizer_antique.transform(query_texts_antique)\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def encode_in_batches(texts, vectorizer, batch_size=BATCH_SIZE):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding queries\", bar_format=\"{l_bar}{bar}\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = vectorizer.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        gc.collect()\n",
    "    return np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "query_embeddings_antique = joblib.load(os.path.join(base_path_antique, \"query_embeddings_matrix_antique.joblib\"))\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù‡Ø¬ÙŠÙ†Ø© Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "import faiss\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        \n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF ÙˆØ§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆÙÙ‡Ø±Ø³ FAISS\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "        index_path = os.path.join(base_path, \"embedding_index.faiss\")\n",
    "        \n",
    "        # ØªØ­Ù…ÙŠÙ„ ÙÙ‡Ø±Ø³ FAISS\n",
    "        if not os.path.exists(index_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Vector store index not found\"}\n",
    "        index = faiss.read_index(index_path)\n",
    "\n",
    "        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… TF-IDF\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ø¹ FAISS\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1).astype(np.float32)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        \n",
    "        # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ù„Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ 10 Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "        distances, indices = index.search(query_vector_embedding, k=10)\n",
    "        doc_indices_embedding = [idx for idx in indices[0] if idx < len(doc_mapping) and idx in filtered_indices]\n",
    "\n",
    "        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØµØ§Ù„Ø­Ø© ÙÙŠ FAISS\n",
    "        if not doc_indices_embedding:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found in FAISS index\"}\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆØ³ÙŠÙ†ÙŠ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© Ù…Ù† FAISS\n",
    "        selected_embeddings = embeddings_matrix[doc_indices_embedding]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, selected_embeddings)\n",
    "        sorted_indices = np.argsort(similarities_embedding[0])[::-1]\n",
    "\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "        for idx in sorted_indices:\n",
    "            original_idx = doc_indices_embedding[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][filtered_indices.index(original_idx)] if original_idx in filtered_indices else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        \n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ranking error: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    \n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            if result['status'] == 'success':\n",
    "                results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']]\n",
    "            else:\n",
    "                results[query_id] = []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù†Ø®ÙØ¶\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù‡Ø¬ÙŠÙ† ANTIQUE\n",
    "results_antique = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_antique, query_embeddings_antique, tfidf_matrix_antique, embeddings_matrix_antique,\n",
    "    queries_df_antique, BATCH_SIZE, doc_id_mapping_antique, base_path_antique\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels\n",
    "qrels_antique = load_qrels(pd.read_csv(qrels_path_antique))\n",
    "\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "del query_tfidf_antique,query_embeddings_antique, \n",
    "del tfidf_matrix_antique, embeddings_matrix_antique, \n",
    "del model_antique, \n",
    "gc.collect()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35676f",
   "metadata": {},
   "source": [
    "hybrid and vector store BEIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef24dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "ğŸ”„ Processing queries (beir): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [56:36<00:00, 53.91s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beir - MAP: 0.6614, Precision@10: 0.1354, Recall@10: 0.7571, MRR: 0.6867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯\n",
    "BATCH_SIZE = 16\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# Paths\n",
    "base_path_beir = r\"data\\beir\"\n",
    "tfidf_matrix_path_beir = os.path.join(base_path_beir, \"tfidf_matrix.joblib\")\n",
    "queries_path_beir = os.path.join(base_path_beir, \"queries_beir.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_beir = os.path.join(base_path_beir, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_beir = os.path.join(base_path_beir, \"embeddings_vectorizer.joblib\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "# Load data\n",
    "docs_beir = pd.read_csv(docs_path_beir)\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "\n",
    "tfidf_matrix_beir = joblib.load(tfidf_matrix_path_beir)\n",
    "embeddings_matrix_beir = joblib.load(embeddings_matrix_path_beir)\n",
    "queries_df_beir = pd.read_csv(queries_path_beir)\n",
    "queries_df_beir = queries_df_beir[queries_df_beir['text'].notna() & queries_df_beir['text'].str.strip() != '']\n",
    "\n",
    "tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "model_beir = joblib.load(embeddings_vectorizer_path_beir)\n",
    "\n",
    "query_texts_beir = queries_df_beir['text'].tolist()\n",
    "query_tfidf_beir = tfidf_vectorizer_beir.transform(query_texts_beir)\n",
    "query_embeddings_beir = joblib.load(os.path.join(base_path_beir, \"query_embeddings_matrix_beir.joblib\"))\n",
    "\n",
    "# Hybrid ranking function\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "        index_path = os.path.join(base_path, \"embedding_index.faiss\")\n",
    "\n",
    "        if not os.path.exists(index_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Vector store index not found\"}\n",
    "        index = faiss.read_index(index_path)\n",
    "\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1).astype(np.float32)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        distances, indices = index.search(query_vector_embedding, k=10)\n",
    "        doc_indices_embedding = [idx for idx in indices[0] if idx < len(doc_mapping) and idx in filtered_indices]\n",
    "        if not doc_indices_embedding:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found in FAISS index\"}\n",
    "\n",
    "        selected_embeddings = embeddings_matrix[doc_indices_embedding]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, selected_embeddings)\n",
    "        sorted_indices = np.argsort(similarities_embedding[0])[::-1]\n",
    "\n",
    "        for idx in sorted_indices:\n",
    "            original_idx = doc_indices_embedding[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][filtered_indices.index(original_idx)] if original_idx in filtered_indices else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Batch processing\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']] if result['status'] == 'success' else []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "# Evaluation functions\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_scores.append(calculate_map(retrieved_docs, relevant_docs))\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "    print(f\"{dataset_name} - MAP: {np.mean(map_scores):.4f}, Precision@10: {np.mean(precision_scores):.4f}, Recall@10: {np.mean(recall_scores):.4f}, MRR: {np.mean(rr_scores):.4f}\")\n",
    "\n",
    "# Run\n",
    "results_beir = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_beir, query_embeddings_beir, tfidf_matrix_beir, embeddings_matrix_beir,\n",
    "    queries_df_beir, BATCH_SIZE, doc_id_mapping_beir, base_path_beir\n",
    ")\n",
    "qrels_beir = load_qrels(pd.read_csv(qrels_path_beir))\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n",
    "\n",
    "# Clean memory\n",
    "del query_tfidf_beir, query_embeddings_beir\n",
    "del tfidf_matrix_beir, embeddings_matrix_beir\n",
    "del model_beir\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1d200",
   "metadata": {},
   "source": [
    "## query and hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e11cbb",
   "metadata": {},
   "source": [
    "## HYBRID AND QUERY ANTIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f334c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for antique\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|\n",
      "ğŸ”„ Processing queries (antique): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:32<00:00, 42.60s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antique - MAP: 0.4024, Precision@10: 0.1960, Recall@10: 0.0648, MRR: 0.4627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3487"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ BATCH_SIZE\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NLTK Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø± Ø§Ù„Ù…Ø®ØµØµØ©\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ doc_ids Ù…Ù† inverted index\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "base_path_antique = r\"data\\antique\"\n",
    "\n",
    "\n",
    "tfidf_matrix_path_antique = os.path.join(base_path_antique, \"tfidf_matrix.joblib\")\n",
    "enhanced_queries_path_antique = os.path.join(base_path_antique, \"enhanced_queries_antique.csv\")\n",
    "docs_path_antique = os.path.join(base_path_antique, \"docs_antique.csv\")\n",
    "doc_id_mapping_path_antique = os.path.join(base_path_antique, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_antique = os.path.join(base_path_antique, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_antique = os.path.join(base_path_antique, \"embeddings_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "qrels_path_antique = os.path.join(base_path_antique, \"qrels_antique.csv\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ doc_id_mapping\n",
    "docs_antique = pd.read_csv(docs_path_antique)\n",
    "\n",
    "doc_id_mapping_antique = {i: str(doc_id) for i, doc_id in enumerate(docs_antique['doc_id'])}\n",
    "\n",
    "joblib.dump(doc_id_mapping_antique, doc_id_mapping_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF Ùˆ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "tfidf_matrix_antique = joblib.load(tfidf_matrix_path_antique)\n",
    "\n",
    "embeddings_matrix_antique = joblib.load(embeddings_matrix_path_antique)\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "queries_df_antique = pd.read_csv(enhanced_queries_path_antique)\n",
    "\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©\n",
    "queries_df_antique = queries_df_antique[queries_df_antique['text'].notna() & queries_df_antique['text'].str.strip() != '']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ÙˆÙ„Ø§Øª TF-IDF\n",
    "tfidf_vectorizer_antique = joblib.load(os.path.join(base_path_antique, \"tfidf_vectorizer.joblib\"))\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ embeddings_vectorizer Ø§Ù„Ù…Ø®Ø²Ù†\n",
    "model_antique = joblib.load(embeddings_vectorizer_path_antique)\n",
    "\n",
    "\n",
    "query_texts_antique = queries_df_antique['text'].tolist()\n",
    "\n",
    "query_tfidf_antique = tfidf_vectorizer_antique.transform(query_texts_antique)\n",
    "\n",
    "\n",
    "queries_df_antique['processed_text'] = queries_df_antique['processed_query'].apply(lambda x: x.split())\n",
    "def get_model(dataset_name):\n",
    "    print(f\"Loading model for {dataset_name}\")\n",
    "    return SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "model_antique = get_model(\"antique\")\n",
    "\n",
    "\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„Ø§Øª embeddings\n",
    "def encode_in_batches(texts, model, batch_size):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding queries\",bar_format='{l_bar}{bar}|'):\n",
    "        batch_texts = [' '.join(text) if isinstance(text, list) else text for text in texts[i:i + batch_size]]\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        gc.collect()\n",
    "    return np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "\n",
    "query_embeddings_antique = encode_in_batches(queries_df_antique['processed_text'].tolist(), model_antique, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù‡Ø¬ÙŠÙ†Ø© Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        \n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ§Øª TF-IDF ÙˆØ§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "\n",
    "        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… TF-IDF\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        filtered_embeddings = embeddings_matrix[filtered_indices]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, filtered_embeddings)\n",
    "        doc_indices_embedding = np.argsort(similarities_embedding[0])[::-1][:10]\n",
    "\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "        for idx in doc_indices_embedding:\n",
    "            original_idx = filtered_indices[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][idx] if idx < len(similarities_tfidf[0]) else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        \n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ranking error: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙØ¹ÙŠ\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    \n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            if result['status'] == 'success':\n",
    "                results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']]\n",
    "            else:\n",
    "                results[query_id] = []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ \n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù‡Ø¬ÙŠÙ† ANTIQUE\n",
    "results_antique = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_antique, query_embeddings_antique, tfidf_matrix_antique, embeddings_matrix_antique,\n",
    "    queries_df_antique, BATCH_SIZE, doc_id_mapping_antique, base_path_antique\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels\n",
    "qrels_antique = load_qrels(pd.read_csv(qrels_path_antique))\n",
    "\n",
    "\n",
    "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "evaluate_results(results_antique, qrels_antique, queries_df_antique, 'antique')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    "del query_tfidf_antique,query_embeddings_antique, \n",
    "del tfidf_matrix_antique, embeddings_matrix_antique, \n",
    "del model_antique, \n",
    "gc.collect()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c75152",
   "metadata": {},
   "source": [
    "## HYBRID AND QUERY BEIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24adb7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for beir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|\n",
      "ğŸ”„ Processing queries (beir): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beir - MAP: 0.5291, Precision@10: 0.1333, Recall@10: 0.2540, MRR: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sqlite3\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "from datetime import datetime\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'never'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def get_doc_ids_from_index(processed_query, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    doc_ids = set()\n",
    "    for term in processed_query:\n",
    "        cursor.execute(\"SELECT doc_id FROM inverted_index WHERE term = ?\", (term,))\n",
    "        doc_ids.update(row[0] for row in cursor.fetchall())\n",
    "    conn.close()\n",
    "    return doc_ids\n",
    "\n",
    "base_path_beir = r\"data\\beir\"\n",
    "\n",
    "tfidf_matrix_path_beir = os.path.join(base_path_beir, \"tfidf_matrix.joblib\")\n",
    "enhanced_queries_path_beir = os.path.join(base_path_beir, \"enhanced_queries_beir.csv\")\n",
    "docs_path_beir = os.path.join(base_path_beir, \"docs_beir.csv\")\n",
    "doc_id_mapping_path_beir = os.path.join(base_path_beir, \"doc_id_mapping.joblib\")\n",
    "embeddings_matrix_path_beir = os.path.join(base_path_beir, \"embeddings_matrix.joblib\")\n",
    "embeddings_vectorizer_path_beir = os.path.join(base_path_beir, \"embeddings_vectorizer.joblib\")\n",
    "qrels_path_beir = os.path.join(base_path_beir, \"qrels_beir.csv\")\n",
    "\n",
    "docs_beir = pd.read_csv(docs_path_beir)\n",
    "doc_id_mapping_beir = {i: str(doc_id) for i, doc_id in enumerate(docs_beir['doc_id'])}\n",
    "joblib.dump(doc_id_mapping_beir, doc_id_mapping_path_beir)\n",
    "\n",
    "tfidf_matrix_beir = joblib.load(tfidf_matrix_path_beir)\n",
    "embeddings_matrix_beir = joblib.load(embeddings_matrix_path_beir)\n",
    "\n",
    "queries_df_beir = pd.read_csv(enhanced_queries_path_beir)\n",
    "queries_df_beir = queries_df_beir[queries_df_beir['text'].notna() & queries_df_beir['text'].str.strip() != '']\n",
    "\n",
    "tfidf_vectorizer_beir = joblib.load(os.path.join(base_path_beir, \"tfidf_vectorizer.joblib\"))\n",
    "model_beir = joblib.load(embeddings_vectorizer_path_beir)\n",
    "\n",
    "query_texts_beir = queries_df_beir['text'].tolist()\n",
    "query_tfidf_beir = tfidf_vectorizer_beir.transform(query_texts_beir)\n",
    "queries_df_beir['processed_text'] = queries_df_beir['processed_query'].apply(\n",
    "    lambda x: x.split() if isinstance(x, str) else []\n",
    ")\n",
    "\n",
    "\n",
    "def get_model(dataset_name):\n",
    "    print(f\"Loading model for {dataset_name}\")\n",
    "    return SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "model_beir = get_model(\"beir\")\n",
    "\n",
    "def encode_in_batches(texts, model, batch_size):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding queries\", bar_format='{l_bar}{bar}|'):\n",
    "        batch_texts = [' '.join(text) if isinstance(text, list) else text for text in texts[i:i + batch_size]]\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=False, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        gc.collect()\n",
    "    return np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "query_embeddings_beir = encode_in_batches(queries_df_beir['processed_text'].tolist(), model_beir, BATCH_SIZE)\n",
    "\n",
    "def rank_documents_hybrid(query_vector, dataset_name):\n",
    "    try:\n",
    "        ranked_docs = []\n",
    "        base_path = f\"data/{dataset_name}\"\n",
    "        db_path = os.path.join(base_path, \"index.db\")\n",
    "        if not os.path.exists(db_path):\n",
    "            return {\"status\": \"error\", \"message\": \"Database not found\"}\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT doc_id, text FROM documents\")\n",
    "        doc_mapping = {i: (doc_id, text) for i, (doc_id, text) in enumerate(cursor.fetchall())}\n",
    "        conn.close()\n",
    "\n",
    "        tfidf_matrix = joblib.load(os.path.join(base_path, \"tfidf_matrix.joblib\"))\n",
    "        embeddings_matrix = joblib.load(os.path.join(base_path, \"embeddings_matrix.joblib\"))\n",
    "        doc_id_mapping = joblib.load(os.path.join(base_path, \"doc_id_mapping.joblib\"))\n",
    "\n",
    "        processed_terms = custom_tokenizer(query_vector['text'])\n",
    "        doc_ids = get_doc_ids_from_index(processed_terms, db_path)\n",
    "        filtered_indices = [idx for idx, doc_id in doc_id_mapping.items() if doc_id in doc_ids]\n",
    "        if not filtered_indices:\n",
    "            return {\"status\": \"error\", \"message\": \"No documents found in inverted index\"}\n",
    "\n",
    "        query_vector_tfidf = np.array(query_vector['tfidf']).reshape(1, -1)\n",
    "        if query_vector_tfidf.shape[1] != tfidf_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in TF-IDF\"}\n",
    "        filtered_tfidf_matrix = tfidf_matrix[filtered_indices]\n",
    "        similarities_tfidf = cosine_similarity(query_vector_tfidf, filtered_tfidf_matrix)\n",
    "        doc_indices_tfidf = np.argsort(similarities_tfidf[0])[::-1][:1000]\n",
    "\n",
    "        query_vector_embedding = np.array(query_vector['embedding']).reshape(1, -1)\n",
    "        if query_vector_embedding.shape[1] != embeddings_matrix.shape[1]:\n",
    "            return {\"status\": \"error\", \"message\": \"Dimension mismatch in embeddings\"}\n",
    "        filtered_embeddings = embeddings_matrix[filtered_indices]\n",
    "        similarities_embedding = cosine_similarity(query_vector_embedding, filtered_embeddings)\n",
    "        doc_indices_embedding = np.argsort(similarities_embedding[0])[::-1][:10]\n",
    "\n",
    "        for idx in doc_indices_embedding:\n",
    "            original_idx = filtered_indices[idx]\n",
    "            if original_idx < tfidf_matrix.shape[0] and original_idx in doc_mapping:\n",
    "                doc_id, text = doc_mapping[original_idx]\n",
    "                tfidf_score = similarities_tfidf[0][idx] if idx < len(similarities_tfidf[0]) else 0.0\n",
    "                ranked_docs.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"score\": float(similarities_embedding[0][idx]),\n",
    "                    \"text\": text,\n",
    "                    \"tfidf_score\": float(tfidf_score)\n",
    "                })\n",
    "\n",
    "        if not ranked_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No valid documents found\"}\n",
    "        \n",
    "        return {\"status\": \"success\", \"ranked_docs\": ranked_docs}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ranking error: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def process_hybrid_similarities_in_batches(query_tfidf, query_embeddings, tfidf_matrix, embeddings_matrix, queries_df, batch_size, doc_id_mapping, dataset_base_path):\n",
    "    results = {}\n",
    "    index_db_path = os.path.join(dataset_base_path, \"index.db\")\n",
    "    \n",
    "    total_batches = (query_tfidf.shape[0] + batch_size - 1) // batch_size\n",
    "    progress_bar = tqdm(total=total_batches, desc=f\"ğŸ”„ Processing queries ({dataset_base_path.split(os.sep)[-1]})\", bar_format=\"{l_bar}{bar}\")\n",
    "\n",
    "    for i in range(0, query_tfidf.shape[0], batch_size):\n",
    "        batch_queries_tfidf = query_tfidf[i:i + batch_size]\n",
    "        batch_queries_embeddings = query_embeddings[i:i + batch_size]\n",
    "        batch_texts = queries_df['text'].iloc[i:i + batch_size].tolist()\n",
    "        batch_query_ids = queries_df['query_id'].iloc[i:i + batch_size].astype(str).tolist()\n",
    "\n",
    "        for query_tfidf_vec, query_emb_vec, query_text, query_id in zip(batch_queries_tfidf, batch_queries_embeddings, batch_texts, batch_query_ids):\n",
    "            query_vector = {\n",
    "                'tfidf': query_tfidf_vec.toarray()[0],\n",
    "                'embedding': query_emb_vec,\n",
    "                'text': query_text\n",
    "            }\n",
    "            result = rank_documents_hybrid(query_vector, dataset_base_path.split(os.sep)[-1])\n",
    "            if result['status'] == 'success':\n",
    "                results[query_id] = [doc['doc_id'] for doc in result['ranked_docs']]\n",
    "            else:\n",
    "                results[query_id] = []\n",
    "        \n",
    "        gc.collect()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return results\n",
    "\n",
    "def calculate_precision_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    return relevant_in_top_k / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    relevant_in_top_k = sum(1 for doc_id in retrieved_docs[:k] if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0)\n",
    "    total_relevant = sum(1 for score in relevant_docs.values() if float(score) > 0)\n",
    "    return relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "def calculate_rr(retrieved_docs, relevant_docs):\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_map(retrieved_docs, relevant_docs):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_docs, 1):\n",
    "        if str(doc_id) in relevant_docs and float(relevant_docs[str(doc_id)]) > 0:\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / i\n",
    "    return precision_sum / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "def load_qrels(qrels_df):\n",
    "    qrels = defaultdict(dict)\n",
    "    for _, row in qrels_df.iterrows():\n",
    "        qrels[str(row['query_id'])][str(row['doc_id'])] = float(row['relevance'])\n",
    "    return qrels\n",
    "\n",
    "def evaluate_results(results, qrels, queries_df, dataset_name):\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    rr_scores = []\n",
    "    low_map_queries = []\n",
    "\n",
    "    for query_id in results:\n",
    "        retrieved_docs = results[query_id]\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        if relevant_docs:\n",
    "            map_score = calculate_map(retrieved_docs, relevant_docs)\n",
    "            map_scores.append(map_score)\n",
    "            if map_score < 0.1:\n",
    "                filtered_df = queries_df[queries_df['query_id'].astype(str) == str(query_id)]\n",
    "                if not filtered_df.empty:\n",
    "                    query_text = filtered_df['text'].iloc[0]\n",
    "                    low_map_queries.append((query_id, query_text, map_score))\n",
    "                else:\n",
    "                    print(f\"Query ID {query_id} not found in queries_df for {dataset_name}\")\n",
    "            precision_scores.append(calculate_precision_at_k(retrieved_docs, relevant_docs))\n",
    "            recall_scores.append(calculate_recall_at_k(retrieved_docs, relevant_docs))\n",
    "            rr_scores.append(calculate_rr(retrieved_docs, relevant_docs))\n",
    "\n",
    "    map_score = np.mean(map_scores) if map_scores else 0.0\n",
    "    precision_score = np.mean(precision_scores) if precision_scores else 0.0\n",
    "    recall_score = np.mean(recall_scores) if recall_scores else 0.0\n",
    "    mrr_score = np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "    print(f\"{dataset_name} - MAP: {map_score:.4f}, Precision@10: {precision_score:.4f}, Recall@10: {recall_score:.4f}, MRR: {mrr_score:.4f}\")\n",
    "\n",
    "results_beir = process_hybrid_similarities_in_batches(\n",
    "    query_tfidf_beir, query_embeddings_beir, tfidf_matrix_beir, embeddings_matrix_beir,\n",
    "    queries_df_beir, BATCH_SIZE, doc_id_mapping_beir, base_path_beir\n",
    ")\n",
    "\n",
    "qrels_beir = load_qrels(pd.read_csv(qrels_path_beir))\n",
    "\n",
    "evaluate_results(results_beir, qrels_beir, queries_df_beir, 'beir')\n",
    "\n",
    "del query_tfidf_beir, query_embeddings_beir\n",
    "del tfidf_matrix_beir, embeddings_matrix_beir\n",
    "del model_beir\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
